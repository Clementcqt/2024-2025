
==> Audit <==
|---------|--------------------------------|----------|------------------|---------|---------------------|---------------------|
| Command |              Args              | Profile  |       User       | Version |     Start Time      |      End Time       |
|---------|--------------------------------|----------|------------------|---------|---------------------|---------------------|
| start   |                                | minikube | PC-CLEMENT\caqcl | v1.34.0 | 20 Nov 24 08:11 CET | 20 Nov 24 08:15 CET |
| start   | --driver=docker                | minikube | PC-CLEMENT\caqcl | v1.34.0 | 27 Nov 24 10:06 CET | 27 Nov 24 10:07 CET |
| service | nginx -n dev                   | minikube | PC-CLEMENT\caqcl | v1.34.0 | 27 Nov 24 10:18 CET | 27 Nov 24 10:27 CET |
| service | nginx -n prod                  | minikube | PC-CLEMENT\caqcl | v1.34.0 | 27 Nov 24 10:32 CET |                     |
| service | fronted -n prod                | minikube | PC-CLEMENT\caqcl | v1.34.0 | 27 Nov 24 10:32 CET |                     |
| service | frontend -n prod               | minikube | PC-CLEMENT\caqcl | v1.34.0 | 27 Nov 24 10:32 CET |                     |
| service | frontend                       | minikube | PC-CLEMENT\caqcl | v1.34.0 | 27 Nov 24 10:36 CET |                     |
| service | -n prod frontend               | minikube | PC-CLEMENT\caqcl | v1.34.0 | 27 Nov 24 10:36 CET |                     |
| service | frontend                       | minikube | PC-CLEMENT\caqcl | v1.34.0 | 27 Nov 24 10:38 CET |                     |
| service | frontend-84cb8dc79f-8dp6n      | minikube | PC-CLEMENT\caqcl | v1.34.0 | 27 Nov 24 10:39 CET |                     |
| service | -n prod                        | minikube | PC-CLEMENT\caqcl | v1.34.0 | 27 Nov 24 10:39 CET |                     |
|         | frontend-84cb8dc79f-8dp6n      |          |                  |         |                     |                     |
| service | -n prod frontend               | minikube | PC-CLEMENT\caqcl | v1.34.0 | 27 Nov 24 10:39 CET |                     |
| service | -n prod frontend               | minikube | PC-CLEMENT\caqcl | v1.34.0 | 27 Nov 24 10:42 CET |                     |
| service | -n prod backend                | minikube | PC-CLEMENT\caqcl | v1.34.0 | 27 Nov 24 10:43 CET |                     |
| service | -n prod frontend               | minikube | PC-CLEMENT\caqcl | v1.34.0 | 27 Nov 24 10:49 CET |                     |
| start   |                                | minikube | PC-CLEMENT\caqcl | v1.34.0 | 27 Nov 24 10:51 CET | 27 Nov 24 10:52 CET |
| service | -n prod frontend               | minikube | PC-CLEMENT\caqcl | v1.34.0 | 27 Nov 24 10:52 CET |                     |
| service | -n prod frontend               | minikube | PC-CLEMENT\caqcl | v1.34.0 | 27 Nov 24 10:53 CET |                     |
|---------|--------------------------------|----------|------------------|---------|---------------------|---------------------|


==> Dernier d√©marrage <==
Log file created at: 2024/11/27 10:51:17
Running on machine: PC-CLEMENT
Binary: Built with gc go1.22.5 for windows/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I1127 10:51:17.182287   48020 out.go:345] Setting OutFile to fd 92 ...
I1127 10:51:17.210134   48020 out.go:358] Setting ErrFile to fd 96...
W1127 10:51:17.272339   48020 root.go:314] Error reading config file at C:\Users\caqcl\.minikube\config\config.json: open C:\Users\caqcl\.minikube\config\config.json: Le fichier sp√©cifi√© est introuvable.
I1127 10:51:17.279323   48020 out.go:352] Setting JSON to false
I1127 10:51:17.305302   48020 start.go:129] hostinfo: {"hostname":"PC-CLEMENT","uptime":595895,"bootTime":1732105181,"procs":321,"os":"windows","platform":"Microsoft Windows 11 Home","platformFamily":"Standalone Workstation","platformVersion":"10.0.22631.4460 Build 22631.4460","kernelVersion":"10.0.22631.4460 Build 22631.4460","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"d8e6cf19-4439-4ac6-be1a-8fb5c561139e"}
W1127 10:51:17.305302   48020 start.go:137] gopshost.Virtualization returned error: not implemented yet
I1127 10:51:17.309254   48020 out.go:177] üòÑ  minikube v1.34.0 sur Microsoft Windows 11 Home 10.0.22631.4460 Build 22631.4460
I1127 10:51:17.317933   48020 notify.go:220] Checking for updates...
I1127 10:51:17.321326   48020 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.31.0
I1127 10:51:17.325831   48020 driver.go:394] Setting default libvirt URI to qemu:///system
I1127 10:51:17.595892   48020 docker.go:123] docker version: linux-27.3.1:Docker Desktop 4.35.1 (173168)
I1127 10:51:17.598649   48020 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1127 10:51:19.840038   48020 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (2.2408808s)
I1127 10:51:19.842414   48020 info.go:266] docker info: {ID:288ec055-260e-41ad-b2dd-f7502f547359 Containers:11 ContainersRunning:1 ContainersPaused:0 ContainersStopped:10 Images:10 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:66 OomKillDisable:true NGoroutines:90 SystemTime:2024-11-27 09:51:19.817963215 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:14 KernelVersion:5.15.167.4-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:22 MemTotal:8155758592 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:27.3.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:472731909fa34bd7bc9c087e4c27943f9835f111 Expected:472731909fa34bd7bc9c087e4c27943f9835f111} RuncCommit:{ID:v1.1.13-0-g58aa920 Expected:v1.1.13-0-g58aa920} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.17.1-desktop.1] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.29.7-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.37] map[Name:desktop Path:C:\Program Files\Docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands (Alpha) Vendor:Docker Inc. Version:v0.0.15] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.27] map[Name:feedback Path:C:\Program Files\Docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.5] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.3.0] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.14.0]] Warnings:<nil>}}
I1127 10:51:19.843510   48020 out.go:177] ‚ú®  Utilisation du pilote docker bas√© sur le profil existant
I1127 10:51:19.845358   48020 start.go:297] selected driver: docker
I1127 10:51:19.845358   48020 start.go:901] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\caqcl:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1127 10:51:19.845358   48020 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1127 10:51:19.852321   48020 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1127 10:51:20.126318   48020 info.go:266] docker info: {ID:288ec055-260e-41ad-b2dd-f7502f547359 Containers:11 ContainersRunning:1 ContainersPaused:0 ContainersStopped:10 Images:10 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:66 OomKillDisable:true NGoroutines:90 SystemTime:2024-11-27 09:51:20.107666265 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:14 KernelVersion:5.15.167.4-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:22 MemTotal:8155758592 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:27.3.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:472731909fa34bd7bc9c087e4c27943f9835f111 Expected:472731909fa34bd7bc9c087e4c27943f9835f111} RuncCommit:{ID:v1.1.13-0-g58aa920 Expected:v1.1.13-0-g58aa920} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.17.1-desktop.1] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.29.7-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.37] map[Name:desktop Path:C:\Program Files\Docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands (Alpha) Vendor:Docker Inc. Version:v0.0.15] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.27] map[Name:feedback Path:C:\Program Files\Docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.5] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.3.0] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.14.0]] Warnings:<nil>}}
I1127 10:51:20.198964   48020 cni.go:84] Creating CNI manager for ""
I1127 10:51:20.198964   48020 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1127 10:51:20.199764   48020 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\caqcl:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1127 10:51:20.200400   48020 out.go:177] üëç  D√©marrage du n≈ìud "minikube" primary control-plane dans le cluster "minikube"
I1127 10:51:20.203338   48020 cache.go:121] Beginning downloading kic base image for docker with docker
I1127 10:51:20.203338   48020 out.go:177] üöú  Extraction de l'image de base v0.0.45...
I1127 10:51:20.204411   48020 preload.go:131] Checking if preload exists for k8s version v1.31.0 and runtime docker
I1127 10:51:20.204411   48020 preload.go:146] Found local preload: C:\Users\caqcl\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.31.0-docker-overlay2-amd64.tar.lz4
I1127 10:51:20.204411   48020 cache.go:56] Caching tarball of preloaded images
I1127 10:51:20.205048   48020 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 in local docker daemon
I1127 10:51:20.205789   48020 preload.go:172] Found C:\Users\caqcl\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.31.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I1127 10:51:20.206295   48020 cache.go:59] Finished verifying existence of preloaded tar for v1.31.0 on docker
I1127 10:51:20.206418   48020 profile.go:143] Saving config to C:\Users\caqcl\.minikube\profiles\minikube\config.json ...
I1127 10:51:20.421916   48020 cache.go:149] Downloading gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 to local cache
I1127 10:51:20.424201   48020 localpath.go:151] windows sanitize: C:\Users\caqcl\.minikube\cache\kic\amd64\kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85.tar -> C:\Users\caqcl\.minikube\cache\kic\amd64\kicbase_v0.0.45@sha256_81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85.tar
I1127 10:51:20.424708   48020 localpath.go:151] windows sanitize: C:\Users\caqcl\.minikube\cache\kic\amd64\kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85.tar -> C:\Users\caqcl\.minikube\cache\kic\amd64\kicbase_v0.0.45@sha256_81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85.tar
I1127 10:51:20.425241   48020 image.go:63] Checking for gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 in local cache directory
I1127 10:51:20.425241   48020 image.go:66] Found gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 in local cache directory, skipping pull
I1127 10:51:20.425783   48020 image.go:135] gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 exists in cache, skipping pull
I1127 10:51:20.425783   48020 cache.go:152] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 as a tarball
I1127 10:51:20.425783   48020 cache.go:162] Loading gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 from local cache
I1127 10:51:20.425783   48020 localpath.go:151] windows sanitize: C:\Users\caqcl\.minikube\cache\kic\amd64\kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85.tar -> C:\Users\caqcl\.minikube\cache\kic\amd64\kicbase_v0.0.45@sha256_81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85.tar
I1127 10:51:39.146249   48020 cache.go:164] successfully loaded and using gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 from cached tarball
I1127 10:51:39.146249   48020 cache.go:194] Successfully downloaded all kic artifacts
I1127 10:51:39.148936   48020 start.go:360] acquireMachinesLock for minikube: {Name:mk792855763047c4b15b28c9d20162f1c2369f11 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1127 10:51:39.149623   48020 start.go:364] duration metric: took 687¬µs to acquireMachinesLock for "minikube"
I1127 10:51:39.149623   48020 start.go:96] Skipping create...Using existing machine configuration
I1127 10:51:39.150381   48020 fix.go:54] fixHost starting: 
I1127 10:51:39.169002   48020 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1127 10:51:39.281277   48020 fix.go:112] recreateIfNeeded on minikube: state=Running err=<nil>
W1127 10:51:39.281277   48020 fix.go:138] unexpected machine state, will restart: <nil>
I1127 10:51:39.283024   48020 out.go:177] üèÉ  Mise √† jour du container docker en marche "minikube" ...
I1127 10:51:39.283599   48020 machine.go:93] provisionDockerMachine start ...
I1127 10:51:39.290639   48020 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1127 10:51:39.393484   48020 main.go:141] libmachine: Using SSH client type: native
I1127 10:51:39.394575   48020 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x80c9c0] 0x80f5a0 <nil>  [] 0s} 127.0.0.1 57588 <nil> <nil>}
I1127 10:51:39.394575   48020 main.go:141] libmachine: About to run SSH command:
hostname
I1127 10:51:39.575305   48020 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1127 10:51:39.575850   48020 ubuntu.go:169] provisioning hostname "minikube"
I1127 10:51:39.583932   48020 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1127 10:51:39.677545   48020 main.go:141] libmachine: Using SSH client type: native
I1127 10:51:39.678090   48020 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x80c9c0] 0x80f5a0 <nil>  [] 0s} 127.0.0.1 57588 <nil> <nil>}
I1127 10:51:39.678090   48020 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I1127 10:51:39.836043   48020 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1127 10:51:39.839878   48020 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1127 10:51:39.923427   48020 main.go:141] libmachine: Using SSH client type: native
I1127 10:51:39.923427   48020 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x80c9c0] 0x80f5a0 <nil>  [] 0s} 127.0.0.1 57588 <nil> <nil>}
I1127 10:51:39.923427   48020 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I1127 10:51:40.083973   48020 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1127 10:51:40.087056   48020 ubuntu.go:175] set auth options {CertDir:C:\Users\caqcl\.minikube CaCertPath:C:\Users\caqcl\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\caqcl\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\caqcl\.minikube\machines\server.pem ServerKeyPath:C:\Users\caqcl\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\caqcl\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\caqcl\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\caqcl\.minikube}
I1127 10:51:40.087056   48020 ubuntu.go:177] setting up certificates
I1127 10:51:40.087056   48020 provision.go:84] configureAuth start
I1127 10:51:40.092006   48020 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1127 10:51:40.164555   48020 provision.go:143] copyHostCerts
I1127 10:51:40.167072   48020 exec_runner.go:144] found C:\Users\caqcl\.minikube/ca.pem, removing ...
I1127 10:51:40.167072   48020 exec_runner.go:203] rm: C:\Users\caqcl\.minikube\ca.pem
I1127 10:51:40.167072   48020 exec_runner.go:151] cp: C:\Users\caqcl\.minikube\certs\ca.pem --> C:\Users\caqcl\.minikube/ca.pem (1074 bytes)
I1127 10:51:40.169894   48020 exec_runner.go:144] found C:\Users\caqcl\.minikube/cert.pem, removing ...
I1127 10:51:40.169894   48020 exec_runner.go:203] rm: C:\Users\caqcl\.minikube\cert.pem
I1127 10:51:40.169894   48020 exec_runner.go:151] cp: C:\Users\caqcl\.minikube\certs\cert.pem --> C:\Users\caqcl\.minikube/cert.pem (1119 bytes)
I1127 10:51:40.171163   48020 exec_runner.go:144] found C:\Users\caqcl\.minikube/key.pem, removing ...
I1127 10:51:40.171163   48020 exec_runner.go:203] rm: C:\Users\caqcl\.minikube\key.pem
I1127 10:51:40.171163   48020 exec_runner.go:151] cp: C:\Users\caqcl\.minikube\certs\key.pem --> C:\Users\caqcl\.minikube/key.pem (1675 bytes)
I1127 10:51:40.173240   48020 provision.go:117] generating server cert: C:\Users\caqcl\.minikube\machines\server.pem ca-key=C:\Users\caqcl\.minikube\certs\ca.pem private-key=C:\Users\caqcl\.minikube\certs\ca-key.pem org=caqcl.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I1127 10:51:40.476583   48020 provision.go:177] copyRemoteCerts
I1127 10:51:40.484227   48020 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I1127 10:51:40.487037   48020 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1127 10:51:40.563631   48020 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:57588 SSHKeyPath:C:\Users\caqcl\.minikube\machines\minikube\id_rsa Username:docker}
I1127 10:51:40.681047   48020 ssh_runner.go:362] scp C:\Users\caqcl\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1074 bytes)
I1127 10:51:40.726622   48020 ssh_runner.go:362] scp C:\Users\caqcl\.minikube\machines\server.pem --> /etc/docker/server.pem (1176 bytes)
I1127 10:51:40.756311   48020 ssh_runner.go:362] scp C:\Users\caqcl\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I1127 10:51:40.789395   48020 provision.go:87] duration metric: took 702.3382ms to configureAuth
I1127 10:51:40.789395   48020 ubuntu.go:193] setting minikube options for container-runtime
I1127 10:51:40.790605   48020 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.31.0
I1127 10:51:40.794150   48020 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1127 10:51:40.884948   48020 main.go:141] libmachine: Using SSH client type: native
I1127 10:51:40.885454   48020 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x80c9c0] 0x80f5a0 <nil>  [] 0s} 127.0.0.1 57588 <nil> <nil>}
I1127 10:51:40.885454   48020 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I1127 10:51:41.074810   48020 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I1127 10:51:41.074810   48020 ubuntu.go:71] root file system type: overlay
I1127 10:51:41.075931   48020 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I1127 10:51:41.081489   48020 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1127 10:51:41.156194   48020 main.go:141] libmachine: Using SSH client type: native
I1127 10:51:41.156702   48020 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x80c9c0] 0x80f5a0 <nil>  [] 0s} 127.0.0.1 57588 <nil> <nil>}
I1127 10:51:41.156702   48020 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I1127 10:51:41.368344   48020 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I1127 10:51:41.377408   48020 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1127 10:51:41.578939   48020 main.go:141] libmachine: Using SSH client type: native
I1127 10:51:41.580219   48020 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x80c9c0] 0x80f5a0 <nil>  [] 0s} 127.0.0.1 57588 <nil> <nil>}
I1127 10:51:41.580219   48020 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I1127 10:51:41.752886   48020 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1127 10:51:41.752886   48020 machine.go:96] duration metric: took 2.4692866s to provisionDockerMachine
I1127 10:51:41.752886   48020 start.go:293] postStartSetup for "minikube" (driver="docker")
I1127 10:51:41.752886   48020 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I1127 10:51:41.767265   48020 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I1127 10:51:41.772051   48020 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1127 10:51:41.874553   48020 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:57588 SSHKeyPath:C:\Users\caqcl\.minikube\machines\minikube\id_rsa Username:docker}
I1127 10:51:41.998554   48020 ssh_runner.go:195] Run: cat /etc/os-release
I1127 10:51:42.005492   48020 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I1127 10:51:42.005492   48020 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I1127 10:51:42.005492   48020 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I1127 10:51:42.006000   48020 info.go:137] Remote host: Ubuntu 22.04.4 LTS
I1127 10:51:42.006000   48020 filesync.go:126] Scanning C:\Users\caqcl\.minikube\addons for local assets ...
I1127 10:51:42.006539   48020 filesync.go:126] Scanning C:\Users\caqcl\.minikube\files for local assets ...
I1127 10:51:42.006539   48020 start.go:296] duration metric: took 253.6533ms for postStartSetup
I1127 10:51:42.014916   48020 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I1127 10:51:42.018416   48020 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1127 10:51:42.136813   48020 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:57588 SSHKeyPath:C:\Users\caqcl\.minikube\machines\minikube\id_rsa Username:docker}
I1127 10:51:42.266575   48020 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I1127 10:51:42.275095   48020 fix.go:56] duration metric: took 3.1247135s for fixHost
I1127 10:51:42.275095   48020 start.go:83] releasing machines lock for "minikube", held for 3.125472s
I1127 10:51:42.280498   48020 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1127 10:51:42.360912   48020 ssh_runner.go:195] Run: curl.exe -sS -m 2 https://registry.k8s.io/
I1127 10:51:42.367985   48020 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1127 10:51:42.369080   48020 ssh_runner.go:195] Run: cat /version.json
I1127 10:51:42.374236   48020 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1127 10:51:42.458459   48020 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:57588 SSHKeyPath:C:\Users\caqcl\.minikube\machines\minikube\id_rsa Username:docker}
I1127 10:51:42.483121   48020 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:57588 SSHKeyPath:C:\Users\caqcl\.minikube\machines\minikube\id_rsa Username:docker}
W1127 10:51:42.580658   48020 start.go:867] [curl.exe -sS -m 2 https://registry.k8s.io/] failed: curl.exe -sS -m 2 https://registry.k8s.io/: Process exited with status 127
stdout:

stderr:
bash: line 1: curl.exe: command not found
I1127 10:51:42.609109   48020 ssh_runner.go:195] Run: systemctl --version
I1127 10:51:42.627542   48020 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I1127 10:51:42.645867   48020 ssh_runner.go:195] Run: sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
W1127 10:51:42.661809   48020 start.go:439] unable to name loopback interface in configureRuntimes: unable to patch loopback cni config "/etc/cni/net.d/*loopback.conf*": sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;: Process exited with status 1
stdout:

stderr:
find: '\\etc\\cni\\net.d': No such file or directory
I1127 10:51:42.668927   48020 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I1127 10:51:42.692734   48020 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I1127 10:51:42.692789   48020 start.go:495] detecting cgroup driver to use...
I1127 10:51:42.692789   48020 detect.go:187] detected "cgroupfs" cgroup driver on host os
I1127 10:51:42.695710   48020 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I1127 10:51:42.725780   48020 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I1127 10:51:42.755154   48020 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I1127 10:51:42.771802   48020 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I1127 10:51:42.779003   48020 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I1127 10:51:42.799104   48020 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1127 10:51:42.817919   48020 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I1127 10:51:42.836416   48020 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1127 10:51:42.857078   48020 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I1127 10:51:42.877969   48020 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
W1127 10:51:42.897866   48020 out.go:270] ‚ùó  Failing to connect to https://registry.k8s.io/ from inside the minikube container
W1127 10:51:42.898414   48020 out.go:270] üí°  Pour extraire de nouvelles images externes, vous devrez peut-√™tre configurer un proxy¬†: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
I1127 10:51:42.903950   48020 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I1127 10:51:42.940832   48020 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I1127 10:51:42.965904   48020 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I1127 10:51:42.993176   48020 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I1127 10:51:43.017745   48020 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1127 10:51:43.274633   48020 ssh_runner.go:195] Run: sudo systemctl restart containerd
I1127 10:51:53.843104   48020 ssh_runner.go:235] Completed: sudo systemctl restart containerd: (10.5681601s)
I1127 10:51:53.843104   48020 start.go:495] detecting cgroup driver to use...
I1127 10:51:53.843104   48020 detect.go:187] detected "cgroupfs" cgroup driver on host os
I1127 10:51:53.858254   48020 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I1127 10:51:53.879348   48020 cruntime.go:279] skipping containerd shutdown because we are bound to it
I1127 10:51:53.890125   48020 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I1127 10:51:53.910910   48020 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I1127 10:51:53.960702   48020 ssh_runner.go:195] Run: which cri-dockerd
I1127 10:51:53.984535   48020 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I1127 10:51:53.998526   48020 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I1127 10:51:54.043853   48020 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I1127 10:51:54.193998   48020 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I1127 10:51:54.348865   48020 docker.go:574] configuring docker to use "cgroupfs" as cgroup driver...
I1127 10:51:54.350594   48020 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I1127 10:51:54.448527   48020 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1127 10:51:54.658437   48020 ssh_runner.go:195] Run: sudo systemctl restart docker
I1127 10:51:55.512966   48020 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I1127 10:51:55.546398   48020 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I1127 10:51:55.588996   48020 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I1127 10:51:55.618130   48020 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I1127 10:51:55.755933   48020 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1127 10:51:55.891023   48020 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1127 10:51:56.034616   48020 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I1127 10:51:56.075611   48020 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I1127 10:51:56.106392   48020 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1127 10:51:56.246840   48020 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I1127 10:51:56.434846   48020 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I1127 10:51:56.448187   48020 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I1127 10:51:56.456238   48020 start.go:563] Will wait 60s for crictl version
I1127 10:51:56.468478   48020 ssh_runner.go:195] Run: which crictl
I1127 10:51:56.489075   48020 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I1127 10:51:56.551172   48020 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  27.2.0
RuntimeApiVersion:  v1
I1127 10:51:56.557062   48020 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1127 10:51:56.607034   48020 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1127 10:51:56.643980   48020 out.go:235] üê≥  Pr√©paration de Kubernetes v1.31.0 sur Docker 27.2.0...
I1127 10:51:56.650600   48020 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I1127 10:51:56.876614   48020 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I1127 10:51:56.890391   48020 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I1127 10:51:56.903848   48020 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1127 10:51:57.002308   48020 kubeadm.go:883] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\caqcl:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I1127 10:51:57.002308   48020 preload.go:131] Checking if preload exists for k8s version v1.31.0 and runtime docker
I1127 10:51:57.008694   48020 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1127 10:51:57.130413   48020 docker.go:685] Got preloaded images: -- stdout --
nginx:latest
registry.k8s.io/kube-controller-manager:v1.31.0
registry.k8s.io/kube-scheduler:v1.31.0
registry.k8s.io/kube-apiserver:v1.31.0
registry.k8s.io/kube-proxy:v1.31.0
registry.k8s.io/etcd:3.5.15-0
registry.k8s.io/pause:3.10
registry.k8s.io/coredns/coredns:v1.11.1
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1127 10:51:57.130413   48020 docker.go:615] Images already preloaded, skipping extraction
I1127 10:51:57.140118   48020 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1127 10:51:57.252067   48020 docker.go:685] Got preloaded images: -- stdout --
nginx:latest
registry.k8s.io/kube-apiserver:v1.31.0
registry.k8s.io/kube-scheduler:v1.31.0
registry.k8s.io/kube-controller-manager:v1.31.0
registry.k8s.io/kube-proxy:v1.31.0
registry.k8s.io/etcd:3.5.15-0
registry.k8s.io/pause:3.10
registry.k8s.io/coredns/coredns:v1.11.1
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1127 10:51:57.252580   48020 cache_images.go:84] Images are preloaded, skipping loading
I1127 10:51:57.252580   48020 kubeadm.go:934] updating node { 192.168.49.2 8443 v1.31.0 docker true true} ...
I1127 10:51:57.254201   48020 kubeadm.go:946] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.31.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I1127 10:51:57.260326   48020 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I1127 10:51:59.137549   48020 ssh_runner.go:235] Completed: docker info --format {{.CgroupDriver}}: (1.8772229s)
I1127 10:51:59.141100   48020 cni.go:84] Creating CNI manager for ""
I1127 10:51:59.141100   48020 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1127 10:51:59.141616   48020 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I1127 10:51:59.142346   48020 kubeadm.go:181] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.31.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I1127 10:51:59.142346   48020 kubeadm.go:187] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.31.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I1127 10:51:59.154727   48020 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.31.0
I1127 10:51:59.232823   48020 binaries.go:44] Found k8s binaries, skipping transfer
I1127 10:51:59.244677   48020 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I1127 10:51:59.425261   48020 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I1127 10:51:59.625982   48020 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I1127 10:51:59.820993   48020 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2150 bytes)
I1127 10:52:00.041151   48020 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I1127 10:52:00.142206   48020 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1127 10:52:01.136955   48020 ssh_runner.go:195] Run: sudo systemctl start kubelet
I1127 10:52:01.235864   48020 certs.go:68] Setting up C:\Users\caqcl\.minikube\profiles\minikube for IP: 192.168.49.2
I1127 10:52:01.235864   48020 certs.go:194] generating shared ca certs ...
I1127 10:52:01.236467   48020 certs.go:226] acquiring lock for ca certs: {Name:mk917a51e535c24cf25ff92a6a8af7290ea4d567 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1127 10:52:01.237860   48020 certs.go:235] skipping valid "minikubeCA" ca cert: C:\Users\caqcl\.minikube\ca.key
I1127 10:52:01.239475   48020 certs.go:235] skipping valid "proxyClientCA" ca cert: C:\Users\caqcl\.minikube\proxy-client-ca.key
I1127 10:52:01.239981   48020 certs.go:256] generating profile certs ...
I1127 10:52:01.241253   48020 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": C:\Users\caqcl\.minikube\profiles\minikube\client.key
I1127 10:52:01.242433   48020 certs.go:359] skipping valid signed profile cert regeneration for "minikube": C:\Users\caqcl\.minikube\profiles\minikube\apiserver.key.7fb57e3c
I1127 10:52:01.243499   48020 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": C:\Users\caqcl\.minikube\profiles\minikube\proxy-client.key
I1127 10:52:01.244804   48020 certs.go:484] found cert: C:\Users\caqcl\.minikube\certs\ca-key.pem (1679 bytes)
I1127 10:52:01.245329   48020 certs.go:484] found cert: C:\Users\caqcl\.minikube\certs\ca.pem (1074 bytes)
I1127 10:52:01.245578   48020 certs.go:484] found cert: C:\Users\caqcl\.minikube\certs\cert.pem (1119 bytes)
I1127 10:52:01.245578   48020 certs.go:484] found cert: C:\Users\caqcl\.minikube\certs\key.pem (1675 bytes)
I1127 10:52:01.251203   48020 ssh_runner.go:362] scp C:\Users\caqcl\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I1127 10:52:01.414391   48020 ssh_runner.go:362] scp C:\Users\caqcl\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I1127 10:52:01.525093   48020 ssh_runner.go:362] scp C:\Users\caqcl\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I1127 10:52:01.638888   48020 ssh_runner.go:362] scp C:\Users\caqcl\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I1127 10:52:01.830215   48020 ssh_runner.go:362] scp C:\Users\caqcl\.minikube\profiles\minikube\apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I1127 10:52:02.017007   48020 ssh_runner.go:362] scp C:\Users\caqcl\.minikube\profiles\minikube\apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I1127 10:52:02.126076   48020 ssh_runner.go:362] scp C:\Users\caqcl\.minikube\profiles\minikube\proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I1127 10:52:02.323874   48020 ssh_runner.go:362] scp C:\Users\caqcl\.minikube\profiles\minikube\proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I1127 10:52:02.520462   48020 ssh_runner.go:362] scp C:\Users\caqcl\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I1127 10:52:02.630674   48020 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I1127 10:52:02.737766   48020 ssh_runner.go:195] Run: openssl version
I1127 10:52:02.846397   48020 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I1127 10:52:02.943602   48020 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I1127 10:52:03.015769   48020 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Nov 20 07:15 /usr/share/ca-certificates/minikubeCA.pem
I1127 10:52:03.029919   48020 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I1127 10:52:03.071943   48020 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I1127 10:52:03.140870   48020 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I1127 10:52:03.162039   48020 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I1127 10:52:03.229202   48020 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I1127 10:52:03.256784   48020 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I1127 10:52:03.338042   48020 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I1127 10:52:03.432795   48020 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I1127 10:52:03.489866   48020 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I1127 10:52:03.526995   48020 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\caqcl:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1127 10:52:03.533611   48020 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1127 10:52:03.640287   48020 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I1127 10:52:03.719106   48020 kubeadm.go:408] found existing configuration files, will attempt cluster restart
I1127 10:52:03.720180   48020 kubeadm.go:593] restartPrimaryControlPlane start ...
I1127 10:52:03.732764   48020 ssh_runner.go:195] Run: sudo test -d /data/minikube
I1127 10:52:03.752391   48020 kubeadm.go:130] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I1127 10:52:03.759537   48020 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1127 10:52:03.856212   48020 kubeconfig.go:125] found "minikube" server: "https://127.0.0.1:57587"
I1127 10:52:03.875070   48020 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I1127 10:52:03.923795   48020 kubeadm.go:630] The running cluster does not require reconfiguration: 127.0.0.1
I1127 10:52:03.925053   48020 kubeadm.go:597] duration metric: took 203.6154ms to restartPrimaryControlPlane
I1127 10:52:03.925640   48020 kubeadm.go:394] duration metric: took 398.6446ms to StartCluster
I1127 10:52:03.925640   48020 settings.go:142] acquiring lock: {Name:mk5cbb6c831714098c43b51a50f35d40d9e226b0 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1127 10:52:03.925640   48020 settings.go:150] Updating kubeconfig:  C:\Users\caqcl\.kube\config
I1127 10:52:03.929252   48020 lock.go:35] WriteFile acquiring C:\Users\caqcl\.kube\config: {Name:mk7f8f792ece254e285a17bf224aa0dc41fe9272 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1127 10:52:03.933519   48020 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.31.0
I1127 10:52:03.933726   48020 addons.go:507] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I1127 10:52:03.933726   48020 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I1127 10:52:03.933726   48020 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I1127 10:52:03.933726   48020 addons.go:69] Setting default-storageclass=true in profile "minikube"
I1127 10:52:03.934341   48020 addons.go:234] Setting addon storage-provisioner=true in "minikube"
W1127 10:52:03.934341   48020 addons.go:243] addon storage-provisioner should already be in state true
I1127 10:52:03.934341   48020 out.go:177] üîé  V√©rification des composants Kubernetes...
I1127 10:52:03.934341   48020 host.go:66] Checking if "minikube" exists ...
I1127 10:52:03.934954   48020 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I1127 10:52:03.947763   48020 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1127 10:52:03.949685   48020 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1127 10:52:03.952640   48020 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1127 10:52:04.026571   48020 out.go:177]     ‚ñ™ Utilisation de l'image gcr.io/k8s-minikube/storage-provisioner:v5
I1127 10:52:04.028548   48020 addons.go:431] installing /etc/kubernetes/addons/storage-provisioner.yaml
I1127 10:52:04.028548   48020 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I1127 10:52:04.034332   48020 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1127 10:52:04.036869   48020 addons.go:234] Setting addon default-storageclass=true in "minikube"
W1127 10:52:04.036869   48020 addons.go:243] addon default-storageclass should already be in state true
I1127 10:52:04.037509   48020 host.go:66] Checking if "minikube" exists ...
I1127 10:52:04.049004   48020 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1127 10:52:04.117766   48020 addons.go:431] installing /etc/kubernetes/addons/storageclass.yaml
I1127 10:52:04.117766   48020 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I1127 10:52:04.117766   48020 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:57588 SSHKeyPath:C:\Users\caqcl\.minikube\machines\minikube\id_rsa Username:docker}
I1127 10:52:04.125330   48020 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1127 10:52:04.198811   48020 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:57588 SSHKeyPath:C:\Users\caqcl\.minikube\machines\minikube\id_rsa Username:docker}
I1127 10:52:04.350642   48020 ssh_runner.go:195] Run: sudo systemctl start kubelet
I1127 10:52:04.427824   48020 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1127 10:52:04.446192   48020 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I1127 10:52:04.467611   48020 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I1127 10:52:04.509990   48020 api_server.go:52] waiting for apiserver process to appear ...
I1127 10:52:04.524744   48020 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1127 10:52:07.585136   48020 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (3.1389443s)
I1127 10:52:07.585136   48020 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (3.1175247s)
I1127 10:52:07.587237   48020 ssh_runner.go:235] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (3.0611441s)
I1127 10:52:07.587904   48020 api_server.go:72] duration metric: took 3.6535112s to wait for apiserver process to appear ...
I1127 10:52:07.587904   48020 api_server.go:88] waiting for apiserver healthz status ...
I1127 10:52:07.589262   48020 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:57587/healthz ...
I1127 10:52:07.612069   48020 api_server.go:279] https://127.0.0.1:57587/healthz returned 200:
ok
I1127 10:52:07.619573   48020 api_server.go:141] control plane version: v1.31.0
I1127 10:52:07.619573   48020 api_server.go:131] duration metric: took 31.6694ms to wait for apiserver health ...
I1127 10:52:07.620319   48020 system_pods.go:43] waiting for kube-system pods to appear ...
I1127 10:52:07.646432   48020 out.go:177] üåü  Modules activ√©s: storage-provisioner, default-storageclass
I1127 10:52:07.647676   48020 addons.go:510] duration metric: took 3.7139497s for enable addons: enabled=[storage-provisioner default-storageclass]
I1127 10:52:07.653926   48020 system_pods.go:59] 7 kube-system pods found
I1127 10:52:07.653926   48020 system_pods.go:61] "coredns-6f6b679f8f-t7dd5" [8988a6a8-f01f-40a4-901f-98465f71453c] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I1127 10:52:07.653926   48020 system_pods.go:61] "etcd-minikube" [bc632fbe-0b0b-495d-bbd6-08db9fe59acf] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I1127 10:52:07.653926   48020 system_pods.go:61] "kube-apiserver-minikube" [bb5489e7-de0c-495c-985a-cc61393bd4b4] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I1127 10:52:07.653926   48020 system_pods.go:61] "kube-controller-manager-minikube" [67cacec6-8c55-436d-9124-bb07c46f4863] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I1127 10:52:07.653926   48020 system_pods.go:61] "kube-proxy-njmkr" [e492fec3-1add-4301-a139-814e669dcc6c] Running
I1127 10:52:07.653926   48020 system_pods.go:61] "kube-scheduler-minikube" [68b44664-1d5e-4c15-ac9c-4c34221eee67] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I1127 10:52:07.653926   48020 system_pods.go:61] "storage-provisioner" [4cd61f82-ca99-482a-ab26-4c48b647912f] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I1127 10:52:07.653926   48020 system_pods.go:74] duration metric: took 33.6074ms to wait for pod list to return data ...
I1127 10:52:07.653926   48020 kubeadm.go:582] duration metric: took 3.7202003s to wait for: map[apiserver:true system_pods:true]
I1127 10:52:07.653926   48020 node_conditions.go:102] verifying NodePressure condition ...
I1127 10:52:07.668036   48020 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I1127 10:52:07.668036   48020 node_conditions.go:123] node cpu capacity is 22
I1127 10:52:07.668722   48020 node_conditions.go:105] duration metric: took 14.7954ms to run NodePressure ...
I1127 10:52:07.668722   48020 start.go:241] waiting for startup goroutines ...
I1127 10:52:07.668722   48020 start.go:246] waiting for cluster config update ...
I1127 10:52:07.668722   48020 start.go:255] writing updated cluster config ...
I1127 10:52:07.685335   48020 ssh_runner.go:195] Run: rm -f paused
I1127 10:52:08.032347   48020 start.go:600] kubectl: 1.30.2, cluster: 1.31.0 (minor skew: 1)
I1127 10:52:08.032920   48020 out.go:177] üèÑ  Termin√© ! kubectl est maintenant configur√© pour utiliser "minikube" cluster et espace de noms "default" par d√©faut.


==> Docker <==
Nov 27 09:51:54 minikube dockerd[14821]: time="2024-11-27T09:51:54.908897558Z" level=info msg="API listen on [::]:2376"
Nov 27 09:51:54 minikube dockerd[14821]: time="2024-11-27T09:51:54.912509936Z" level=info msg="stopping event stream following graceful shutdown" error="<nil>" module=libcontainerd namespace=moby
Nov 27 09:51:54 minikube dockerd[14821]: time="2024-11-27T09:51:54.914297206Z" level=info msg="Daemon shutdown complete"
Nov 27 09:51:54 minikube dockerd[14821]: time="2024-11-27T09:51:54.914488230Z" level=info msg="stopping event stream following graceful shutdown" error="context canceled" module=libcontainerd namespace=plugins.moby
Nov 27 09:51:54 minikube systemd[1]: docker.service: Deactivated successfully.
Nov 27 09:51:54 minikube systemd[1]: Stopped Docker Application Container Engine.
Nov 27 09:51:54 minikube systemd[1]: Starting Docker Application Container Engine...
Nov 27 09:51:54 minikube dockerd[15130]: time="2024-11-27T09:51:54.990672474Z" level=info msg="Starting up"
Nov 27 09:51:55 minikube dockerd[15130]: time="2024-11-27T09:51:55.023954719Z" level=info msg="[graphdriver] trying configured driver: overlay2"
Nov 27 09:51:55 minikube dockerd[15130]: time="2024-11-27T09:51:55.047005582Z" level=info msg="Loading containers: start."
Nov 27 09:51:55 minikube dockerd[15130]: time="2024-11-27T09:51:55.291069463Z" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
Nov 27 09:51:55 minikube dockerd[15130]: time="2024-11-27T09:51:55.351125123Z" level=warning msg="error locating sandbox id f76220f0bf183affa0e50dab8280a1c4a8acdc61d6823418386dd4ef8e8af994: sandbox f76220f0bf183affa0e50dab8280a1c4a8acdc61d6823418386dd4ef8e8af994 not found"
Nov 27 09:51:55 minikube dockerd[15130]: time="2024-11-27T09:51:55.351246536Z" level=warning msg="error locating sandbox id 3b749423d6528e6fae0393edfd3f676d8be12c91c115084b230cc9bf3daf6725: sandbox 3b749423d6528e6fae0393edfd3f676d8be12c91c115084b230cc9bf3daf6725 not found"
Nov 27 09:51:55 minikube dockerd[15130]: time="2024-11-27T09:51:55.351269884Z" level=warning msg="error locating sandbox id 5cc1def7f137b25dbbda147d7c2331827d69fb17080cd6bf2caf727db663ed72: sandbox 5cc1def7f137b25dbbda147d7c2331827d69fb17080cd6bf2caf727db663ed72 not found"
Nov 27 09:51:55 minikube dockerd[15130]: time="2024-11-27T09:51:55.351290532Z" level=warning msg="error locating sandbox id 754863db9979c64cc4d3a9756edddb3a8e42f3bfa05f40d1cbc079c98fb17ab3: sandbox 754863db9979c64cc4d3a9756edddb3a8e42f3bfa05f40d1cbc079c98fb17ab3 not found"
Nov 27 09:51:55 minikube dockerd[15130]: time="2024-11-27T09:51:55.351309858Z" level=warning msg="error locating sandbox id 9700c5b24a916d1b76064d85d535c27851ef26f9e163010b8919b4301890d86f: sandbox 9700c5b24a916d1b76064d85d535c27851ef26f9e163010b8919b4301890d86f not found"
Nov 27 09:51:55 minikube dockerd[15130]: time="2024-11-27T09:51:55.351330895Z" level=warning msg="error locating sandbox id 70052177fcc27fffc5cb94b98a722b6518a0ad99db10881949da7278fe7065ec: sandbox 70052177fcc27fffc5cb94b98a722b6518a0ad99db10881949da7278fe7065ec not found"
Nov 27 09:51:55 minikube dockerd[15130]: time="2024-11-27T09:51:55.351938314Z" level=info msg="Loading containers: done."
Nov 27 09:51:55 minikube dockerd[15130]: time="2024-11-27T09:51:55.406663605Z" level=warning msg="WARNING: No blkio throttle.read_bps_device support"
Nov 27 09:51:55 minikube dockerd[15130]: time="2024-11-27T09:51:55.406737002Z" level=warning msg="WARNING: No blkio throttle.write_bps_device support"
Nov 27 09:51:55 minikube dockerd[15130]: time="2024-11-27T09:51:55.406745870Z" level=warning msg="WARNING: No blkio throttle.read_iops_device support"
Nov 27 09:51:55 minikube dockerd[15130]: time="2024-11-27T09:51:55.406751324Z" level=warning msg="WARNING: No blkio throttle.write_iops_device support"
Nov 27 09:51:55 minikube dockerd[15130]: time="2024-11-27T09:51:55.406781643Z" level=info msg="Docker daemon" commit=3ab5c7d containerd-snapshotter=false storage-driver=overlay2 version=27.2.0
Nov 27 09:51:55 minikube dockerd[15130]: time="2024-11-27T09:51:55.406860379Z" level=info msg="Daemon has completed initialization"
Nov 27 09:51:55 minikube dockerd[15130]: time="2024-11-27T09:51:55.496128183Z" level=info msg="API listen on /var/run/docker.sock"
Nov 27 09:51:55 minikube dockerd[15130]: time="2024-11-27T09:51:55.496288488Z" level=info msg="API listen on [::]:2376"
Nov 27 09:51:55 minikube systemd[1]: Started Docker Application Container Engine.
Nov 27 09:51:55 minikube systemd[1]: Stopping CRI Interface for Docker Application Container Engine...
Nov 27 09:51:55 minikube systemd[1]: cri-docker.service: Deactivated successfully.
Nov 27 09:51:55 minikube systemd[1]: Stopped CRI Interface for Docker Application Container Engine.
Nov 27 09:51:56 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
Nov 27 09:51:56 minikube systemd[1]: cri-docker.service: Deactivated successfully.
Nov 27 09:51:56 minikube systemd[1]: Stopped CRI Interface for Docker Application Container Engine.
Nov 27 09:51:56 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
Nov 27 09:51:56 minikube cri-dockerd[15434]: time="2024-11-27T09:51:56Z" level=info msg="Starting cri-dockerd dev (HEAD)"
Nov 27 09:51:56 minikube cri-dockerd[15434]: time="2024-11-27T09:51:56Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
Nov 27 09:51:56 minikube cri-dockerd[15434]: time="2024-11-27T09:51:56Z" level=info msg="Start docker client with request timeout 0s"
Nov 27 09:51:56 minikube cri-dockerd[15434]: time="2024-11-27T09:51:56Z" level=info msg="Hairpin mode is set to hairpin-veth"
Nov 27 09:51:56 minikube cri-dockerd[15434]: time="2024-11-27T09:51:56Z" level=info msg="Loaded network plugin cni"
Nov 27 09:51:56 minikube cri-dockerd[15434]: time="2024-11-27T09:51:56Z" level=info msg="Docker cri networking managed by network plugin cni"
Nov 27 09:51:56 minikube cri-dockerd[15434]: time="2024-11-27T09:51:56Z" level=info msg="Setting cgroupDriver cgroupfs"
Nov 27 09:51:56 minikube cri-dockerd[15434]: time="2024-11-27T09:51:56Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Nov 27 09:51:56 minikube cri-dockerd[15434]: time="2024-11-27T09:51:56Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Nov 27 09:51:56 minikube cri-dockerd[15434]: time="2024-11-27T09:51:56Z" level=info msg="Start cri-dockerd grpc backend"
Nov 27 09:51:56 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Nov 27 09:51:56 minikube cri-dockerd[15434]: time="2024-11-27T09:51:56Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-6f6b679f8f-t7dd5_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"893fc843935a33636e6f53b883cbbcf33dd313b4f9cbf6a08ca12a31fa5cef3a\""
Nov 27 09:51:56 minikube cri-dockerd[15434]: time="2024-11-27T09:51:56Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-6f6b679f8f-t7dd5_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"e7b013f98fb967c2616e9106393772351fef78c8d60b90319ec6e4ef2fabe4b7\""
Nov 27 09:51:56 minikube cri-dockerd[15434]: time="2024-11-27T09:51:56Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"backend-5ff864f988-skjgt_prod\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"da7e6dae5379a8f53467e8033215423df1b01d786c78f8e3cf635b82bad080f4\""
Nov 27 09:51:57 minikube cri-dockerd[15434]: time="2024-11-27T09:51:57Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"frontend-84cb8dc79f-jw9sj_prod\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"d9b01efb8858c66c2f9fd4462cb9c40198de0c28aa754cee681905217ddea5e7\""
Nov 27 09:51:57 minikube cri-dockerd[15434]: time="2024-11-27T09:51:57Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/e05d0d13ac1e46e85bccaa4a125e0d3c9e0c67527b808c125a6af654665312fd/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Nov 27 09:51:58 minikube cri-dockerd[15434]: time="2024-11-27T09:51:58Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/1caaa9a7b09bf098d26bd3689b0ec756fee079c7cf7de09728e8560c363a45d4/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Nov 27 09:51:58 minikube dockerd[15130]: time="2024-11-27T09:51:58.625670694Z" level=error msg="Failed to compute size of container rootfs ad35b3f2048b270f6550a488f986dbf7ad7d8eade3d8b129153b0eed7ac6707f: mount does not exist"
Nov 27 09:51:58 minikube cri-dockerd[15434]: time="2024-11-27T09:51:58Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/037ea9d9a7e804f4245266ed8cc56d915ce30b9a5f241793ee71a5a70c2fdffc/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Nov 27 09:51:58 minikube cri-dockerd[15434]: time="2024-11-27T09:51:58Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/164c4a748dfff3db77ab386a3100ab621c2081aa5c0f5473f07a5af69efb1e61/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Nov 27 09:51:59 minikube cri-dockerd[15434]: time="2024-11-27T09:51:59Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/a4197b2c317a14fea9ea6a69a31428bd6934878c83f5a29dbdf01008b59cad70/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Nov 27 09:51:59 minikube cri-dockerd[15434]: time="2024-11-27T09:51:59Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d46a50ed87364d2eb028bd35668fb8eafe1377ddfb3e971a7608558878bc9541/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Nov 27 09:51:59 minikube cri-dockerd[15434]: time="2024-11-27T09:51:59Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/761ca96a584761f4338e3733fd9168d27aa9de89f39ae47431634dfa09ffbef2/resolv.conf as [nameserver 10.96.0.10 search prod.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Nov 27 09:51:59 minikube cri-dockerd[15434]: time="2024-11-27T09:51:59Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/98c5df378e9440b779f6f471d6c7a8a88faad2d2f8c50dea693a7c4ef75964a8/resolv.conf as [nameserver 10.96.0.10 search prod.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Nov 27 09:51:59 minikube cri-dockerd[15434]: time="2024-11-27T09:51:59Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/6fc598aa777292203111381a60712782730e07df3d3ed4702964626e98d1de65/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Nov 27 09:52:00 minikube dockerd[15130]: time="2024-11-27T09:52:00.814673832Z" level=info msg="ignoring event" container=e3f12406ccce053f055c8da07ec7eeaff010943ddd42228c1399559948565eb3 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"


==> container status <==
CONTAINER           IMAGE               CREATED              STATE               NAME                      ATTEMPT             POD ID              POD
43cb30f1c9fb7       6e38f40d628db       About a minute ago   Running             storage-provisioner       4                   037ea9d9a7e80       storage-provisioner
90d1167e31967       cbb01a7bd410d       About a minute ago   Running             coredns                   2                   6fc598aa77729       coredns-6f6b679f8f-t7dd5
344685c6ecb81       1766f54c897f0       About a minute ago   Running             kube-scheduler            2                   d46a50ed87364       kube-scheduler-minikube
a3de915259432       2e96e5913fc06       About a minute ago   Running             etcd                      2                   a4197b2c317a1       etcd-minikube
e8f4886d43325       ad83b2ca7b09e       About a minute ago   Running             kube-proxy                2                   164c4a748dfff       kube-proxy-njmkr
e3f12406ccce0       6e38f40d628db       About a minute ago   Exited              storage-provisioner       3                   037ea9d9a7e80       storage-provisioner
30948939b46c9       604f5db92eaa8       About a minute ago   Running             kube-apiserver            2                   1caaa9a7b09bf       kube-apiserver-minikube
f19f361b02fe7       045733566833c       About a minute ago   Running             kube-controller-manager   2                   e05d0d13ac1e4       kube-controller-manager-minikube
98f07e326c2ba       cbb01a7bd410d       46 minutes ago       Exited              coredns                   1                   893fc843935a3       coredns-6f6b679f8f-t7dd5
dc9cd03fb882e       ad83b2ca7b09e       46 minutes ago       Exited              kube-proxy                1                   930484f37e7a0       kube-proxy-njmkr
29d452a1e28be       1766f54c897f0       46 minutes ago       Exited              kube-scheduler            1                   18efe3889099c       kube-scheduler-minikube
b80466966b161       604f5db92eaa8       46 minutes ago       Exited              kube-apiserver            1                   2b84ef2bdfca0       kube-apiserver-minikube
0c5441f87b409       2e96e5913fc06       46 minutes ago       Exited              etcd                      1                   de245df41d4ea       etcd-minikube
78965e60dcd57       045733566833c       46 minutes ago       Exited              kube-controller-manager   1                   4e25cbd2885fb       kube-controller-manager-minikube


==> coredns [90d1167e3196] <==
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
.:53
[INFO] plugin/reload: Running configuration SHA512 = f869070685748660180df1b7a47d58cdafcf2f368266578c062d1151dc2c900964aecc5975e8882e6de6fdfb6460463e30ebfaad2ec8f0c3c6436f80225b3b5b
CoreDNS-1.11.1
linux/amd64, go1.20.7, ae2bbc2
[INFO] 127.0.0.1:51234 - 14493 "HINFO IN 7105010928464476383.2103215771221302421. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.077143023s


==> coredns [98f07e326c2b] <==
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = f869070685748660180df1b7a47d58cdafcf2f368266578c062d1151dc2c900964aecc5975e8882e6de6fdfb6460463e30ebfaad2ec8f0c3c6436f80225b3b5b
CoreDNS-1.11.1
linux/amd64, go1.20.7, ae2bbc2
[INFO] 127.0.0.1:40886 - 43640 "HINFO IN 657631708258097902.9091249579420453846. udp 56 false 512" NXDOMAIN qr,rd,ra 56 0.038264377s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[501494579]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (27-Nov-2024 09:07:17.103) (total time: 21046ms):
Trace[501494579]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 21045ms (09:07:38.146)
Trace[501494579]: [21.046967007s] [21.046967007s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[198050755]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (27-Nov-2024 09:07:17.103) (total time: 21048ms):
Trace[198050755]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 21048ms (09:07:38.149)
Trace[198050755]: [21.048160752s] [21.048160752s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[307944244]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (27-Nov-2024 09:07:17.103) (total time: 21050ms):
Trace[307944244]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 21050ms (09:07:38.151)
Trace[307944244]: [21.050396954s] [21.050396954s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=210b148df93a80eb872ecbeb7e35281b3c582c61
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2024_11_20T08_15_27_0700
                    minikube.k8s.io/version=v1.34.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Wed, 20 Nov 2024 07:15:25 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Wed, 27 Nov 2024 09:53:17 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Wed, 27 Nov 2024 09:51:08 +0000   Wed, 20 Nov 2024 07:15:24 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Wed, 27 Nov 2024 09:51:08 +0000   Wed, 20 Nov 2024 07:15:24 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Wed, 27 Nov 2024 09:51:08 +0000   Wed, 20 Nov 2024 07:15:24 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Wed, 27 Nov 2024 09:51:08 +0000   Wed, 20 Nov 2024 07:15:25 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                22
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             7964608Ki
  pods:               110
Allocatable:
  cpu:                22
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             7964608Ki
  pods:               110
System Info:
  Machine ID:                 917252d7a80440879d60f76d27a21ca8
  System UUID:                917252d7a80440879d60f76d27a21ca8
  Boot ID:                    746adf45-6dd2-497b-8f46-dfe83aca2c04
  Kernel Version:             5.15.167.4-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.4 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://27.2.0
  Kubelet Version:            v1.31.0
  Kube-Proxy Version:         
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (9 in total)
  Namespace                   Name                                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                ------------  ----------  ---------------  -------------  ---
  kube-system                 coredns-6f6b679f8f-t7dd5            100m (0%)     0 (0%)      70Mi (0%)        170Mi (2%)     7d2h
  kube-system                 etcd-minikube                       100m (0%)     0 (0%)      100Mi (1%)       0 (0%)         7d2h
  kube-system                 kube-apiserver-minikube             250m (1%)     0 (0%)      0 (0%)           0 (0%)         7d2h
  kube-system                 kube-controller-manager-minikube    200m (0%)     0 (0%)      0 (0%)           0 (0%)         7d2h
  kube-system                 kube-proxy-njmkr                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         7d2h
  kube-system                 kube-scheduler-minikube             100m (0%)     0 (0%)      0 (0%)           0 (0%)         7d2h
  kube-system                 storage-provisioner                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         7d2h
  prod                        backend-5ff864f988-skjgt            0 (0%)        0 (0%)      0 (0%)           0 (0%)         11m
  prod                        frontend-84cb8dc79f-jw9sj           0 (0%)        0 (0%)      0 (0%)           0 (0%)         10m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (3%)   0 (0%)
  memory             170Mi (2%)  170Mi (2%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type     Reason                             Age                From             Message
  ----     ------                             ----               ----             -------
  Normal   Starting                           46m                kube-proxy       
  Normal   Starting                           73s                kube-proxy       
  Warning  PossibleMemoryBackedVolumesOnDisk  46m                kubelet          The tmpfs noswap option is not supported. Memory-backed volumes (e.g. secrets, emptyDirs, etc.) might be swapped to disk and should no longer be considered secure.
  Normal   Starting                           46m                kubelet          Starting kubelet.
  Warning  CgroupV1                           46m                kubelet          Cgroup v1 support is in maintenance mode, please migrate to Cgroup v2.
  Normal   NodeHasSufficientMemory            46m (x7 over 46m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure              46m (x7 over 46m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID               46m (x7 over 46m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced            46m                kubelet          Updated Node Allocatable limit across pods
  Normal   RegisteredNode                     46m                node-controller  Node minikube event: Registered Node minikube in Controller
  Normal   RegisteredNode                     71s                node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[Nov27 08:47] PCI: Fatal: No config space access function found
[  +0.028542] PCI: System does not support PCI
[  +0.256223] kvm: already loaded the other module
[  +0.918087] FS-Cache: Duplicate cookie detected
[  +0.000437] FS-Cache: O-cookie c=00000005 [p=00000002 fl=222 nc=0 na=1]
[  +0.000520] FS-Cache: O-cookie d=00000000d9b2cfa5{9P.session} n=00000000ece348b8
[  +0.000597] FS-Cache: O-key=[10] '34323934393337343234'
[  +0.000499] FS-Cache: N-cookie c=00000006 [p=00000002 fl=2 nc=0 na=1]
[  +0.000518] FS-Cache: N-cookie d=00000000d9b2cfa5{9P.session} n=000000008d22a3e3
[  +0.000926] FS-Cache: N-key=[10] '34323934393337343234'
[  +0.639392] WSL (1) ERROR: ConfigApplyWindowsLibPath:2531: open /etc/ld.so.conf.d/ld.wsl.conf
[  +0.000007]  failed 2
[  +0.020470] WSL (1) WARNING: /usr/share/zoneinfo/Europe/Paris not found. Is the tzdata package installed?
[  +0.412680] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.015204] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.003192] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.006361] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.006570] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001514] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.005680] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001494] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001242] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001736] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +1.255739] netlink: 'init': attribute type 4 has an invalid length.
[Nov27 09:07] tmpfs: Unknown parameter 'noswap'


==> etcd [0c5441f87b40] <==
{"level":"info","ts":"2024-11-27T09:07:11.849177Z","caller":"etcdserver/server.go:751","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"aec36adc501070cc","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2024-11-27T09:07:11.849367Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2024-11-27T09:07:11.849537Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2024-11-27T09:07:11.849551Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2024-11-27T09:07:11.851264Z","caller":"embed/etcd.go:728","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-11-27T09:07:11.851399Z","caller":"embed/etcd.go:599","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-11-27T09:07:11.851415Z","caller":"embed/etcd.go:571","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-11-27T09:07:11.851950Z","caller":"embed/etcd.go:279","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2024-11-27T09:07:11.852000Z","caller":"embed/etcd.go:870","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2024-11-27T09:07:12.738178Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 2"}
{"level":"info","ts":"2024-11-27T09:07:12.738333Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 2"}
{"level":"info","ts":"2024-11-27T09:07:12.738445Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 2"}
{"level":"info","ts":"2024-11-27T09:07:12.738467Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 3"}
{"level":"info","ts":"2024-11-27T09:07:12.738473Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 3"}
{"level":"info","ts":"2024-11-27T09:07:12.738484Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 3"}
{"level":"info","ts":"2024-11-27T09:07:12.738492Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 3"}
{"level":"info","ts":"2024-11-27T09:07:12.800830Z","caller":"etcdserver/server.go:2118","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2024-11-27T09:07:12.800837Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-11-27T09:07:12.800914Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-11-27T09:07:12.804600Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2024-11-27T09:07:12.805032Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2024-11-27T09:07:12.810848Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2024-11-27T09:07:12.810876Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2024-11-27T09:07:12.812422Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2024-11-27T09:07:12.812581Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2024-11-27T09:17:13.508392Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":11530}
{"level":"info","ts":"2024-11-27T09:17:13.515343Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":11530,"took":"5.91525ms","hash":3395362007,"current-db-size-bytes":1916928,"current-db-size":"1.9 MB","current-db-size-in-use-bytes":1916928,"current-db-size-in-use":"1.9 MB"}
{"level":"info","ts":"2024-11-27T09:17:13.515406Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3395362007,"revision":11530,"compact-revision":10920}
{"level":"info","ts":"2024-11-27T09:22:13.501588Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":11769}
{"level":"info","ts":"2024-11-27T09:22:13.507954Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":11769,"took":"5.850395ms","hash":1151296183,"current-db-size-bytes":1916928,"current-db-size":"1.9 MB","current-db-size-in-use-bytes":1585152,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2024-11-27T09:22:13.508025Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1151296183,"revision":11769,"compact-revision":11530}
{"level":"info","ts":"2024-11-27T09:27:13.494481Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":12050}
{"level":"info","ts":"2024-11-27T09:27:13.500705Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":12050,"took":"5.884047ms","hash":3134187638,"current-db-size-bytes":1916928,"current-db-size":"1.9 MB","current-db-size-in-use-bytes":1769472,"current-db-size-in-use":"1.8 MB"}
{"level":"info","ts":"2024-11-27T09:27:13.500775Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3134187638,"revision":12050,"compact-revision":11769}
{"level":"info","ts":"2024-11-27T09:32:13.491636Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":12316}
{"level":"info","ts":"2024-11-27T09:32:13.497254Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":12316,"took":"4.355203ms","hash":1849538726,"current-db-size-bytes":1916928,"current-db-size":"1.9 MB","current-db-size-in-use-bytes":1671168,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2024-11-27T09:32:13.497357Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1849538726,"revision":12316,"compact-revision":12050}
{"level":"info","ts":"2024-11-27T09:37:13.480376Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":12601}
{"level":"info","ts":"2024-11-27T09:37:13.484321Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":12601,"took":"3.594072ms","hash":1760006173,"current-db-size-bytes":1916928,"current-db-size":"1.9 MB","current-db-size-in-use-bytes":1703936,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2024-11-27T09:37:13.484398Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1760006173,"revision":12601,"compact-revision":12316}
{"level":"info","ts":"2024-11-27T09:42:13.472419Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":12892}
{"level":"info","ts":"2024-11-27T09:42:13.477451Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":12892,"took":"4.692811ms","hash":189989806,"current-db-size-bytes":1916928,"current-db-size":"1.9 MB","current-db-size-in-use-bytes":1736704,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2024-11-27T09:42:13.477501Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":189989806,"revision":12892,"compact-revision":12601}
{"level":"info","ts":"2024-11-27T09:47:13.463923Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":13160}
{"level":"info","ts":"2024-11-27T09:47:13.468932Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":13160,"took":"4.06871ms","hash":3004328896,"current-db-size-bytes":2052096,"current-db-size":"2.1 MB","current-db-size-in-use-bytes":1929216,"current-db-size-in-use":"1.9 MB"}
{"level":"info","ts":"2024-11-27T09:47:13.469032Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3004328896,"revision":13160,"compact-revision":12892}
{"level":"info","ts":"2024-11-27T09:51:43.317102Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2024-11-27T09:51:43.320869Z","caller":"embed/etcd.go:377","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
{"level":"warn","ts":"2024-11-27T09:51:43.522009Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2024-11-27T09:51:43.522702Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2024-11-27T09:51:43.920495Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"191.976278ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/default/kubernetes\" ","response":"","error":"rangeKeys: context cancelled: context canceled"}
{"level":"info","ts":"2024-11-27T09:51:43.920955Z","caller":"traceutil/trace.go:171","msg":"trace[1245200345] range","detail":"{range_begin:/registry/services/endpoints/default/kubernetes; range_end:; }","duration":"398.904662ms","start":"2024-11-27T09:51:43.521872Z","end":"2024-11-27T09:51:43.920776Z","steps":["trace[1245200345] 'range keys from in-memory index tree'  (duration: 191.879523ms)"],"step_count":1}
{"level":"warn","ts":"2024-11-27T09:51:43.921531Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-11-27T09:51:43.521799Z","time spent":"399.585179ms","remote":"127.0.0.1:38050","response type":"/etcdserverpb.KV/Range","request count":0,"request size":49,"response count":0,"response size":0,"request content":"key:\"/registry/services/endpoints/default/kubernetes\" "}
2024/11/27 09:51:43 WARNING: [core] [Server #7] grpc: Server.processUnaryRPC failed to write status: connection error: desc = "transport is closing"
{"level":"warn","ts":"2024-11-27T09:51:44.414501Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"warn","ts":"2024-11-27T09:51:44.414617Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"info","ts":"2024-11-27T09:51:44.415240Z","caller":"etcdserver/server.go:1521","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2024-11-27T09:51:44.519200Z","caller":"embed/etcd.go:581","msg":"stopping serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-11-27T09:51:44.519644Z","caller":"embed/etcd.go:586","msg":"stopped serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-11-27T09:51:44.519682Z","caller":"embed/etcd.go:379","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}


==> etcd [a3de91525943] <==
{"level":"warn","ts":"2024-11-27T09:52:01.318295Z","caller":"embed/config.go:687","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2024-11-27T09:52:01.319914Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://192.168.49.2:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://192.168.49.2:2380","--initial-cluster=minikube=https://192.168.49.2:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.49.2:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.49.2:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"info","ts":"2024-11-27T09:52:01.320878Z","caller":"etcdmain/etcd.go:116","msg":"server has been already initialized","data-dir":"/var/lib/minikube/etcd","dir-type":"member"}
{"level":"warn","ts":"2024-11-27T09:52:01.321190Z","caller":"embed/config.go:687","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2024-11-27T09:52:01.321270Z","caller":"embed/etcd.go:128","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2024-11-27T09:52:01.321574Z","caller":"embed/etcd.go:496","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-11-27T09:52:01.326403Z","caller":"embed/etcd.go:136","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"]}
{"level":"info","ts":"2024-11-27T09:52:01.326856Z","caller":"embed/etcd.go:310","msg":"starting an etcd server","etcd-version":"3.5.15","git-sha":"9a5533382","go-version":"go1.21.12","go-os":"linux","go-arch":"amd64","max-cpu-set":22,"max-cpu-available":22,"member-initialized":true,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"","initial-cluster-state":"new","initial-cluster-token":"","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2024-11-27T09:52:01.330052Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"2.572944ms"}
{"level":"info","ts":"2024-11-27T09:52:01.719816Z","caller":"etcdserver/server.go:511","msg":"recovered v2 store from snapshot","snapshot-index":10001,"snapshot-size":"7.1 kB"}
{"level":"info","ts":"2024-11-27T09:52:01.719981Z","caller":"etcdserver/server.go:524","msg":"recovered v3 backend from snapshot","backend-size-bytes":2576384,"backend-size":"2.6 MB","backend-size-in-use-bytes":1994752,"backend-size-in-use":"2.0 MB"}
{"level":"info","ts":"2024-11-27T09:52:01.930932Z","caller":"etcdserver/raft.go:530","msg":"restarting local member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","commit-index":17048}
{"level":"info","ts":"2024-11-27T09:52:01.932219Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2024-11-27T09:52:01.932847Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 3"}
{"level":"info","ts":"2024-11-27T09:52:01.932908Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [aec36adc501070cc], term: 3, commit: 17048, applied: 10001, lastindex: 17048, lastterm: 3]"}
{"level":"info","ts":"2024-11-27T09:52:01.933230Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2024-11-27T09:52:01.933402Z","caller":"membership/cluster.go:278","msg":"recovered/added member from store","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","recovered-remote-peer-id":"aec36adc501070cc","recovered-remote-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2024-11-27T09:52:01.933456Z","caller":"membership/cluster.go:287","msg":"set cluster version from store","cluster-version":"3.5"}
{"level":"warn","ts":"2024-11-27T09:52:01.935949Z","caller":"auth/store.go:1241","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2024-11-27T09:52:01.939056Z","caller":"mvcc/kvstore.go:341","msg":"restored last compact revision","meta-bucket-name":"meta","meta-bucket-name-key":"finishedCompactRev","restored-compact-revision":13160}
{"level":"info","ts":"2024-11-27T09:52:01.942828Z","caller":"mvcc/kvstore.go:418","msg":"kvstore restored","current-rev":13714}
{"level":"info","ts":"2024-11-27T09:52:02.022503Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2024-11-27T09:52:02.026511Z","caller":"etcdserver/corrupt.go:96","msg":"starting initial corruption check","local-member-id":"aec36adc501070cc","timeout":"7s"}
{"level":"info","ts":"2024-11-27T09:52:02.027235Z","caller":"etcdserver/corrupt.go:177","msg":"initial corruption checking passed; no corruption","local-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2024-11-27T09:52:02.027325Z","caller":"etcdserver/server.go:858","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.15","cluster-id":"fa54960ea34d58be","cluster-version":"3.5"}
{"level":"info","ts":"2024-11-27T09:52:02.027868Z","caller":"etcdserver/server.go:751","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"aec36adc501070cc","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2024-11-27T09:52:02.027933Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2024-11-27T09:52:02.028266Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2024-11-27T09:52:02.028301Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2024-11-27T09:52:02.028406Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2024-11-27T09:52:02.030724Z","caller":"embed/etcd.go:728","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-11-27T09:52:02.031068Z","caller":"embed/etcd.go:279","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2024-11-27T09:52:02.031120Z","caller":"embed/etcd.go:870","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2024-11-27T09:52:02.031117Z","caller":"embed/etcd.go:599","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-11-27T09:52:02.031172Z","caller":"embed/etcd.go:571","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-11-27T09:52:02.235001Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 3"}
{"level":"info","ts":"2024-11-27T09:52:02.235072Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 3"}
{"level":"info","ts":"2024-11-27T09:52:02.235112Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 3"}
{"level":"info","ts":"2024-11-27T09:52:02.235126Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 4"}
{"level":"info","ts":"2024-11-27T09:52:02.235131Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 4"}
{"level":"info","ts":"2024-11-27T09:52:02.235140Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 4"}
{"level":"info","ts":"2024-11-27T09:52:02.235188Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 4"}
{"level":"info","ts":"2024-11-27T09:52:02.241613Z","caller":"etcdserver/server.go:2118","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2024-11-27T09:52:02.242027Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-11-27T09:52:02.242808Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-11-27T09:52:02.316137Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2024-11-27T09:52:02.316201Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2024-11-27T09:52:02.322117Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2024-11-27T09:52:02.323027Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2024-11-27T09:52:02.323253Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2024-11-27T09:52:02.324192Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}


==> kernel <==
 09:53:19 up  1:05,  0 users,  load average: 0.92, 0.70, 0.49
Linux minikube 5.15.167.4-microsoft-standard-WSL2 #1 SMP Tue Nov 5 00:21:55 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.4 LTS"


==> kube-apiserver [30948939b46c] <==
W1127 09:52:04.637050       1 genericapiserver.go:765] Skipping API admissionregistration.k8s.io/v1alpha1 because it has no resources.
I1127 09:52:04.637598       1 handler.go:286] Adding GroupVersion events.k8s.io v1 to ResourceManager
W1127 09:52:04.637607       1 genericapiserver.go:765] Skipping API events.k8s.io/v1beta1 because it has no resources.
I1127 09:52:04.726032       1 handler.go:286] Adding GroupVersion apiregistration.k8s.io v1 to ResourceManager
W1127 09:52:04.726110       1 genericapiserver.go:765] Skipping API apiregistration.k8s.io/v1beta1 because it has no resources.
I1127 09:52:05.475577       1 dynamic_cafile_content.go:160] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I1127 09:52:05.475687       1 dynamic_cafile_content.go:160] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I1127 09:52:05.475874       1 secure_serving.go:213] Serving securely on [::]:8443
I1127 09:52:05.475949       1 dynamic_serving_content.go:135] "Starting controller" name="serving-cert::/var/lib/minikube/certs/apiserver.crt::/var/lib/minikube/certs/apiserver.key"
I1127 09:52:05.476344       1 dynamic_serving_content.go:135] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I1127 09:52:05.476424       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I1127 09:52:05.476509       1 controller.go:119] Starting legacy_token_tracking_controller
I1127 09:52:05.476540       1 shared_informer.go:313] Waiting for caches to sync for configmaps
I1127 09:52:05.476552       1 cluster_authentication_trust_controller.go:443] Starting cluster_authentication_trust_controller controller
I1127 09:52:05.476570       1 shared_informer.go:313] Waiting for caches to sync for cluster_authentication_trust_controller
I1127 09:52:05.476594       1 local_available_controller.go:156] Starting LocalAvailability controller
I1127 09:52:05.476612       1 dynamic_cafile_content.go:160] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I1127 09:52:05.476618       1 cache.go:32] Waiting for caches to sync for LocalAvailability controller
I1127 09:52:05.476663       1 apf_controller.go:377] Starting API Priority and Fairness config controller
I1127 09:52:05.476716       1 dynamic_cafile_content.go:160] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I1127 09:52:05.476673       1 customresource_discovery_controller.go:292] Starting DiscoveryController
I1127 09:52:05.476683       1 system_namespaces_controller.go:66] Starting system namespaces controller
I1127 09:52:05.476731       1 controller.go:78] Starting OpenAPI AggregationController
I1127 09:52:05.476756       1 apiservice_controller.go:100] Starting APIServiceRegistrationController
I1127 09:52:05.476772       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I1127 09:52:05.476830       1 aggregator.go:169] waiting for initial CRD sync...
I1127 09:52:05.476855       1 establishing_controller.go:81] Starting EstablishingController
I1127 09:52:05.476876       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I1127 09:52:05.476903       1 crd_finalizer.go:269] Starting CRDFinalizer
I1127 09:52:05.476928       1 crdregistration_controller.go:114] Starting crd-autoregister controller
I1127 09:52:05.476938       1 shared_informer.go:313] Waiting for caches to sync for crd-autoregister
I1127 09:52:05.476857       1 gc_controller.go:78] Starting apiserver lease garbage collector
I1127 09:52:05.477033       1 controller.go:90] Starting OpenAPI V3 controller
I1127 09:52:05.477055       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I1127 09:52:05.477063       1 naming_controller.go:294] Starting NamingConditionController
I1127 09:52:05.477037       1 controller.go:142] Starting OpenAPI controller
I1127 09:52:05.477222       1 controller.go:80] Starting OpenAPI V3 AggregationController
I1127 09:52:05.477703       1 remote_available_controller.go:411] Starting RemoteAvailability controller
I1127 09:52:05.477738       1 cache.go:32] Waiting for caches to sync for RemoteAvailability controller
I1127 09:52:05.712209       1 apf_controller.go:382] Running API Priority and Fairness config worker
I1127 09:52:05.712291       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I1127 09:52:05.713873       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I1127 09:52:05.713918       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I1127 09:52:05.714833       1 shared_informer.go:320] Caches are synced for crd-autoregister
I1127 09:52:05.714890       1 aggregator.go:171] initial CRD sync complete...
I1127 09:52:05.714948       1 autoregister_controller.go:144] Starting autoregister controller
I1127 09:52:05.714968       1 cache.go:32] Waiting for caches to sync for autoregister controller
I1127 09:52:05.715004       1 policy_source.go:224] refreshing policies
I1127 09:52:05.719886       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I1127 09:52:05.724824       1 shared_informer.go:320] Caches are synced for node_authorizer
I1127 09:52:05.777227       1 shared_informer.go:320] Caches are synced for configmaps
I1127 09:52:05.777414       1 cache.go:39] Caches are synced for LocalAvailability controller
I1127 09:52:05.777933       1 cache.go:39] Caches are synced for RemoteAvailability controller
I1127 09:52:05.778113       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I1127 09:52:05.815442       1 cache.go:39] Caches are synced for autoregister controller
I1127 09:52:05.819801       1 handler_discovery.go:450] Starting ResourceDiscoveryManager
I1127 09:52:06.527642       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I1127 09:52:09.144191       1 controller.go:615] quota admission added evaluator for: endpoints
I1127 09:52:09.245705       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I1127 09:52:09.245706       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io


==> kube-apiserver [b80466966b16] <==
W1127 09:51:49.068383       1 logging.go:55] [core] [Channel #49 SubChannel #50]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1127 09:51:49.113580       1 logging.go:55] [core] [Channel #160 SubChannel #161]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1127 09:51:49.114920       1 logging.go:55] [core] [Channel #133 SubChannel #134]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1127 09:51:49.144787       1 logging.go:55] [core] [Channel #94 SubChannel #95]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1127 09:51:49.148491       1 logging.go:55] [core] [Channel #37 SubChannel #38]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1127 09:51:49.148950       1 logging.go:55] [core] [Channel #28 SubChannel #29]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1127 09:51:49.211525       1 logging.go:55] [core] [Channel #64 SubChannel #65]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1127 09:51:49.238190       1 logging.go:55] [core] [Channel #70 SubChannel #71]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1127 09:51:49.252562       1 logging.go:55] [core] [Channel #100 SubChannel #101]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1127 09:51:49.259920       1 logging.go:55] [core] [Channel #169 SubChannel #170]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1127 09:51:49.265196       1 logging.go:55] [core] [Channel #40 SubChannel #41]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1127 09:51:49.274650       1 logging.go:55] [core] [Channel #181 SubChannel #182]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1127 09:51:49.294911       1 logging.go:55] [core] [Channel #175 SubChannel #176]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1127 09:51:49.300032       1 logging.go:55] [core] [Channel #58 SubChannel #59]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1127 09:51:49.326919       1 logging.go:55] [core] [Channel #139 SubChannel #140]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1127 09:51:49.375923       1 logging.go:55] [core] [Channel #52 SubChannel #53]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1127 09:51:49.407858       1 logging.go:55] [core] [Channel #79 SubChannel #80]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1127 09:51:49.470886       1 logging.go:55] [core] [Channel #115 SubChannel #116]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1127 09:51:49.475401       1 logging.go:55] [core] [Channel #163 SubChannel #164]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1127 09:51:49.492078       1 logging.go:55] [core] [Channel #178 SubChannel #179]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1127 09:51:49.498050       1 logging.go:55] [core] [Channel #31 SubChannel #32]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1127 09:51:49.539951       1 logging.go:55] [core] [Channel #46 SubChannel #47]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1127 09:51:49.541028       1 logging.go:55] [core] [Channel #112 SubChannel #113]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1127 09:51:49.545537       1 logging.go:55] [core] [Channel #61 SubChannel #62]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1127 09:51:49.549278       1 logging.go:55] [core] [Channel #25 SubChannel #26]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1127 09:51:49.558950       1 logging.go:55] [core] [Channel #121 SubChannel #122]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1127 09:51:49.618383       1 logging.go:55] [core] [Channel #154 SubChannel #155]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1127 09:51:49.651309       1 logging.go:55] [core] [Channel #184 SubChannel #185]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1127 09:51:49.655028       1 logging.go:55] [core] [Channel #124 SubChannel #125]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1127 09:51:49.687768       1 logging.go:55] [core] [Channel #142 SubChannel #143]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1127 09:51:49.687842       1 logging.go:55] [core] [Channel #118 SubChannel #119]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1127 09:51:49.708116       1 logging.go:55] [core] [Channel #145 SubChannel #146]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1127 09:51:49.749219       1 logging.go:55] [core] [Channel #73 SubChannel #74]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1127 09:51:51.966754       1 logging.go:55] [core] [Channel #106 SubChannel #107]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1127 09:51:52.241672       1 logging.go:55] [core] [Channel #136 SubChannel #137]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1127 09:51:52.251797       1 logging.go:55] [core] [Channel #166 SubChannel #167]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1127 09:51:52.316909       1 logging.go:55] [core] [Channel #151 SubChannel #152]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1127 09:51:52.601375       1 logging.go:55] [core] [Channel #64 SubChannel #65]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1127 09:51:52.610240       1 logging.go:55] [core] [Channel #55 SubChannel #56]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1127 09:51:52.669379       1 logging.go:55] [core] [Channel #7 SubChannel #8]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1127 09:51:52.672922       1 logging.go:55] [core] [Channel #34 SubChannel #35]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1127 09:51:52.730347       1 logging.go:55] [core] [Channel #109 SubChannel #110]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1127 09:51:52.778821       1 logging.go:55] [core] [Channel #139 SubChannel #140]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1127 09:51:52.792983       1 logging.go:55] [core] [Channel #103 SubChannel #104]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1127 09:51:52.824728       1 logging.go:55] [core] [Channel #82 SubChannel #83]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1127 09:51:52.865606       1 logging.go:55] [core] [Channel #22 SubChannel #23]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1127 09:51:52.882982       1 logging.go:55] [core] [Channel #76 SubChannel #77]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1127 09:51:52.977695       1 logging.go:55] [core] [Channel #28 SubChannel #29]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1127 09:51:52.978498       1 logging.go:55] [core] [Channel #37 SubChannel #38]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1127 09:51:52.988059       1 logging.go:55] [core] [Channel #130 SubChannel #131]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1127 09:51:53.007854       1 logging.go:55] [core] [Channel #127 SubChannel #128]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1127 09:51:53.057876       1 logging.go:55] [core] [Channel #70 SubChannel #71]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1127 09:51:53.064505       1 logging.go:55] [core] [Channel #100 SubChannel #101]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1127 09:51:53.113030       1 logging.go:55] [core] [Channel #118 SubChannel #119]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1127 09:51:53.174158       1 logging.go:55] [core] [Channel #133 SubChannel #134]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1127 09:51:53.196387       1 logging.go:55] [core] [Channel #172 SubChannel #173]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1127 09:51:53.221158       1 logging.go:55] [core] [Channel #148 SubChannel #149]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1127 09:51:53.243713       1 logging.go:55] [core] [Channel #46 SubChannel #47]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1127 09:51:53.290812       1 logging.go:55] [core] [Channel #88 SubChannel #89]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1127 09:51:53.335610       1 logging.go:55] [core] [Channel #91 SubChannel #92]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"


==> kube-controller-manager [78965e60dcd5] <==
I1127 09:07:18.503751       1 shared_informer.go:320] Caches are synced for deployment
I1127 09:07:18.552967       1 shared_informer.go:320] Caches are synced for disruption
I1127 09:07:18.597544       1 shared_informer.go:320] Caches are synced for daemon sets
I1127 09:07:18.605743       1 shared_informer.go:320] Caches are synced for stateful set
I1127 09:07:18.652763       1 shared_informer.go:320] Caches are synced for resource quota
I1127 09:07:18.662145       1 shared_informer.go:320] Caches are synced for resource quota
I1127 09:07:18.813656       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="395.995595ms"
I1127 09:07:18.814712       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="74.745¬µs"
I1127 09:07:19.077183       1 shared_informer.go:320] Caches are synced for garbage collector
I1127 09:07:19.113313       1 shared_informer.go:320] Caches are synced for garbage collector
I1127 09:07:19.113356       1 garbagecollector.go:157] "All resource monitors have synced. Proceeding to collect garbage" logger="garbage-collector-controller"
I1127 09:07:42.100300       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="13.139611ms"
I1127 09:07:42.100732       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="45.409¬µs"
I1127 09:09:50.856384       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="dev/nginx-676b6c5bbc" duration="24.081104ms"
I1127 09:09:50.862350       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="dev/nginx-676b6c5bbc" duration="5.860319ms"
I1127 09:09:50.862564       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="dev/nginx-676b6c5bbc" duration="53.66¬µs"
I1127 09:09:50.876732       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="dev/nginx-676b6c5bbc" duration="33.11¬µs"
I1127 09:10:05.606592       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="dev/nginx-676b6c5bbc" duration="6.287098ms"
I1127 09:10:05.606980       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="dev/nginx-676b6c5bbc" duration="67.994¬µs"
I1127 09:10:18.064063       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1127 09:15:24.310142       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1127 09:20:29.834812       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1127 09:21:27.099013       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="dev/nginx-676b6c5bbc" duration="36.924563ms"
I1127 09:21:27.108585       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="dev/nginx-676b6c5bbc" duration="9.502123ms"
I1127 09:21:27.108672       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="dev/nginx-676b6c5bbc" duration="33.569¬µs"
I1127 09:21:27.113550       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="dev/nginx-676b6c5bbc" duration="55.968¬µs"
I1127 09:21:27.121426       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="dev/nginx-676b6c5bbc" duration="32.593¬µs"
I1127 09:21:33.546611       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="dev/nginx-676b6c5bbc" duration="10.236865ms"
I1127 09:21:33.546790       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="dev/nginx-676b6c5bbc" duration="42.794¬µs"
I1127 09:21:34.575200       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="dev/nginx-676b6c5bbc" duration="9.220521ms"
I1127 09:21:34.575404       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="dev/nginx-676b6c5bbc" duration="131.039¬µs"
I1127 09:23:12.362084       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="dev/nginx-676b6c5bbc" duration="4.338¬µs"
I1127 09:25:36.277532       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1127 09:27:46.480986       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="dev/backend-5ff864f988" duration="20.950364ms"
I1127 09:27:46.490384       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="dev/backend-5ff864f988" duration="9.331688ms"
I1127 09:27:46.490542       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="dev/backend-5ff864f988" duration="64.819¬µs"
I1127 09:27:46.490646       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="dev/backend-5ff864f988" duration="24.978¬µs"
I1127 09:27:46.507904       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="dev/backend-5ff864f988" duration="31.56¬µs"
I1127 09:27:47.258842       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="dev/backend-5ff864f988" duration="40.868¬µs"
I1127 09:30:41.673450       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1127 09:32:19.311346       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="dev/frontend-84cb8dc79f" duration="20.970333ms"
I1127 09:32:19.319852       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="dev/frontend-84cb8dc79f" duration="8.396122ms"
I1127 09:32:19.320012       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="dev/frontend-84cb8dc79f" duration="48.18¬µs"
I1127 09:32:19.324010       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="dev/frontend-84cb8dc79f" duration="33.918¬µs"
I1127 09:32:20.866281       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="dev/frontend-84cb8dc79f" duration="83.604¬µs"
I1127 09:35:48.134580       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1127 09:40:54.978129       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1127 09:41:35.185655       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="dev/frontend-84cb8dc79f" duration="21.708¬µs"
I1127 09:41:46.622805       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="dev/backend-5ff864f988" duration="8.769¬µs"
I1127 09:42:13.498588       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="prod/backend-5ff864f988" duration="19.551244ms"
I1127 09:42:13.517547       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="prod/backend-5ff864f988" duration="18.872467ms"
I1127 09:42:13.517648       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="prod/backend-5ff864f988" duration="36.618¬µs"
I1127 09:42:14.442797       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="prod/backend-5ff864f988" duration="68.936¬µs"
I1127 09:42:40.595775       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="prod/frontend-84cb8dc79f" duration="14.954962ms"
I1127 09:42:40.603652       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="prod/frontend-84cb8dc79f" duration="7.776299ms"
I1127 09:42:40.603812       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="prod/frontend-84cb8dc79f" duration="40.719¬µs"
I1127 09:42:40.613467       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="prod/frontend-84cb8dc79f" duration="44.056¬µs"
I1127 09:42:41.701929       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="prod/frontend-84cb8dc79f" duration="62.756¬µs"
I1127 09:46:00.656279       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1127 09:51:08.465755       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"


==> kube-controller-manager [f19f361b02fe] <==
I1127 09:52:08.891430       1 pvc_protection_controller.go:105] "Starting PVC protection controller" logger="persistentvolumeclaim-protection-controller"
I1127 09:52:08.891479       1 shared_informer.go:313] Waiting for caches to sync for PVC protection
I1127 09:52:08.897219       1 shared_informer.go:313] Waiting for caches to sync for resource quota
I1127 09:52:08.906669       1 actual_state_of_world.go:540] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I1127 09:52:08.917164       1 shared_informer.go:320] Caches are synced for taint-eviction-controller
I1127 09:52:08.918716       1 shared_informer.go:313] Waiting for caches to sync for garbage collector
I1127 09:52:08.920649       1 shared_informer.go:320] Caches are synced for ephemeral
I1127 09:52:08.922312       1 shared_informer.go:320] Caches are synced for daemon sets
I1127 09:52:08.923476       1 shared_informer.go:320] Caches are synced for PV protection
I1127 09:52:08.926878       1 shared_informer.go:320] Caches are synced for legacy-service-account-token-cleaner
I1127 09:52:08.934507       1 shared_informer.go:320] Caches are synced for validatingadmissionpolicy-status
I1127 09:52:08.938386       1 shared_informer.go:320] Caches are synced for service account
I1127 09:52:08.940397       1 shared_informer.go:320] Caches are synced for crt configmap
I1127 09:52:08.941806       1 shared_informer.go:320] Caches are synced for certificate-csrapproving
I1127 09:52:08.942004       1 shared_informer.go:320] Caches are synced for endpoint
I1127 09:52:08.942551       1 shared_informer.go:320] Caches are synced for persistent volume
I1127 09:52:08.945445       1 shared_informer.go:320] Caches are synced for bootstrap_signer
I1127 09:52:08.950035       1 shared_informer.go:320] Caches are synced for namespace
I1127 09:52:08.956682       1 shared_informer.go:320] Caches are synced for attach detach
I1127 09:52:08.962671       1 shared_informer.go:320] Caches are synced for endpoint_slice
I1127 09:52:08.969263       1 shared_informer.go:320] Caches are synced for taint
I1127 09:52:08.969462       1 node_lifecycle_controller.go:1232] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I1127 09:52:08.969580       1 node_lifecycle_controller.go:884] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I1127 09:52:08.969812       1 node_lifecycle_controller.go:1078] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I1127 09:52:08.972118       1 shared_informer.go:320] Caches are synced for expand
I1127 09:52:08.976648       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-client
I1127 09:52:08.976657       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-serving
I1127 09:52:08.976675       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kube-apiserver-client
I1127 09:52:08.976795       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-legacy-unknown
I1127 09:52:08.982300       1 shared_informer.go:320] Caches are synced for ClusterRoleAggregator
I1127 09:52:08.984948       1 shared_informer.go:320] Caches are synced for endpoint_slice_mirroring
I1127 09:52:08.990273       1 shared_informer.go:320] Caches are synced for TTL
I1127 09:52:08.990666       1 shared_informer.go:320] Caches are synced for HPA
I1127 09:52:08.990795       1 shared_informer.go:320] Caches are synced for stateful set
I1127 09:52:08.991120       1 shared_informer.go:320] Caches are synced for GC
I1127 09:52:08.992151       1 shared_informer.go:320] Caches are synced for PVC protection
I1127 09:52:08.995105       1 shared_informer.go:320] Caches are synced for node
I1127 09:52:08.995172       1 range_allocator.go:171] "Sending events to api server" logger="node-ipam-controller"
I1127 09:52:08.995197       1 range_allocator.go:177] "Starting range CIDR allocator" logger="node-ipam-controller"
I1127 09:52:08.995202       1 shared_informer.go:313] Waiting for caches to sync for cidrallocator
I1127 09:52:08.995207       1 shared_informer.go:320] Caches are synced for cidrallocator
I1127 09:52:08.995272       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1127 09:52:09.029485       1 shared_informer.go:320] Caches are synced for ReplicationController
I1127 09:52:09.068341       1 shared_informer.go:320] Caches are synced for ReplicaSet
I1127 09:52:09.068764       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="prod/backend-5ff864f988" duration="177.109¬µs"
I1127 09:52:09.068830       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="prod/frontend-84cb8dc79f" duration="278.168¬µs"
I1127 09:52:09.073459       1 shared_informer.go:320] Caches are synced for disruption
I1127 09:52:09.087142       1 shared_informer.go:320] Caches are synced for deployment
I1127 09:52:09.164915       1 shared_informer.go:320] Caches are synced for job
I1127 09:52:09.197367       1 shared_informer.go:320] Caches are synced for resource quota
I1127 09:52:09.214278       1 shared_informer.go:320] Caches are synced for resource quota
I1127 09:52:09.217469       1 shared_informer.go:320] Caches are synced for TTL after finished
I1127 09:52:09.241712       1 shared_informer.go:320] Caches are synced for cronjob
I1127 09:52:09.300824       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="232.282807ms"
I1127 09:52:09.301251       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="53.392¬µs"
I1127 09:52:09.619383       1 shared_informer.go:320] Caches are synced for garbage collector
I1127 09:52:09.658379       1 shared_informer.go:320] Caches are synced for garbage collector
I1127 09:52:09.658461       1 garbagecollector.go:157] "All resource monitors have synced. Proceeding to collect garbage" logger="garbage-collector-controller"
I1127 09:52:11.921097       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="13.668184ms"
I1127 09:52:11.921569       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="143.857¬µs"


==> kube-proxy [dc9cd03fb882] <==
E1127 09:07:16.990385       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
E1127 09:07:17.008131       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
I1127 09:07:17.037554       1 server_linux.go:66] "Using iptables proxy"
I1127 09:07:17.404269       1 server.go:677] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E1127 09:07:17.405243       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I1127 09:07:17.461990       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I1127 09:07:17.462080       1 server_linux.go:169] "Using iptables Proxier"
I1127 09:07:17.466949       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
E1127 09:07:17.481414       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv4"
E1127 09:07:17.507221       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv6"
I1127 09:07:17.508814       1 server.go:483] "Version info" version="v1.31.0"
I1127 09:07:17.508885       1 server.go:485] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1127 09:07:17.513266       1 config.go:104] "Starting endpoint slice config controller"
I1127 09:07:17.513273       1 config.go:197] "Starting service config controller"
I1127 09:07:17.513305       1 config.go:326] "Starting node config controller"
I1127 09:07:17.516615       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I1127 09:07:17.516778       1 shared_informer.go:313] Waiting for caches to sync for node config
I1127 09:07:17.516783       1 shared_informer.go:313] Waiting for caches to sync for service config
I1127 09:07:17.617232       1 shared_informer.go:320] Caches are synced for endpoint slice config
I1127 09:07:17.617242       1 shared_informer.go:320] Caches are synced for service config
I1127 09:07:17.617281       1 shared_informer.go:320] Caches are synced for node config


==> kube-proxy [e8f4886d4332] <==
E1127 09:52:01.640898       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
E1127 09:52:01.714581       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
I1127 09:52:01.921679       1 server_linux.go:66] "Using iptables proxy"
I1127 09:52:05.822207       1 server.go:677] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E1127 09:52:05.823060       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I1127 09:52:06.223077       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I1127 09:52:06.223585       1 server_linux.go:169] "Using iptables Proxier"
I1127 09:52:06.228319       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
E1127 09:52:06.251487       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv4"
E1127 09:52:06.347100       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv6"
I1127 09:52:06.347665       1 server.go:483] "Version info" version="v1.31.0"
I1127 09:52:06.347686       1 server.go:485] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1127 09:52:06.416635       1 config.go:104] "Starting endpoint slice config controller"
I1127 09:52:06.416677       1 config.go:326] "Starting node config controller"
I1127 09:52:06.416905       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I1127 09:52:06.416966       1 shared_informer.go:313] Waiting for caches to sync for node config
I1127 09:52:06.417114       1 config.go:197] "Starting service config controller"
I1127 09:52:06.417174       1 shared_informer.go:313] Waiting for caches to sync for service config
I1127 09:52:06.517454       1 shared_informer.go:320] Caches are synced for node config
I1127 09:52:06.517811       1 shared_informer.go:320] Caches are synced for service config
I1127 09:52:06.618191       1 shared_informer.go:320] Caches are synced for endpoint slice config


==> kube-scheduler [29d452a1e28b] <==
I1127 09:07:13.111408       1 serving.go:386] Generated self-signed cert in-memory
W1127 09:07:14.903637       1 requestheader_controller.go:196] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W1127 09:07:14.903686       1 authentication.go:370] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W1127 09:07:14.903695       1 authentication.go:371] Continuing without authentication configuration. This may treat all requests as anonymous.
W1127 09:07:14.903701       1 authentication.go:372] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I1127 09:07:15.104135       1 server.go:167] "Starting Kubernetes Scheduler" version="v1.31.0"
I1127 09:07:15.104207       1 server.go:169] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1127 09:07:15.107744       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I1127 09:07:15.107789       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1127 09:07:15.108044       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I1127 09:07:15.110384       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I1127 09:07:15.210715       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I1127 09:51:43.524775       1 tlsconfig.go:258] "Shutting down DynamicServingCertificateController"
E1127 09:51:43.525215       1 run.go:72] "command failed" err="finished without leader elect"


==> kube-scheduler [344685c6ecb8] <==
I1127 09:52:03.539882       1 serving.go:386] Generated self-signed cert in-memory
W1127 09:52:05.717035       1 requestheader_controller.go:196] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W1127 09:52:05.717223       1 authentication.go:370] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W1127 09:52:05.717259       1 authentication.go:371] Continuing without authentication configuration. This may treat all requests as anonymous.
W1127 09:52:05.717364       1 authentication.go:372] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I1127 09:52:05.930932       1 server.go:167] "Starting Kubernetes Scheduler" version="v1.31.0"
I1127 09:52:05.930998       1 server.go:169] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1127 09:52:06.023285       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1127 09:52:06.023581       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I1127 09:52:06.024329       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I1127 09:52:06.024526       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I1127 09:52:06.124862       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kubelet <==
Nov 27 09:52:01 minikube kubelet[1594]: I1127 09:52:01.030398    1594 status_manager.go:851] "Failed to get status for pod" podUID="a5363f4f31e043bdae3c93aca4991903" pod="kube-system/etcd-minikube" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/etcd-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Nov 27 09:52:01 minikube kubelet[1594]: I1127 09:52:01.030732    1594 status_manager.go:851] "Failed to get status for pod" podUID="9e315b3a91fa9f6f7463439d9dac1a56" pod="kube-system/kube-apiserver-minikube" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-apiserver-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Nov 27 09:52:01 minikube kubelet[1594]: I1127 09:52:01.031289    1594 status_manager.go:851] "Failed to get status for pod" podUID="e039200acb850c82bb901653cc38ff6e" pod="kube-system/kube-scheduler-minikube" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-scheduler-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Nov 27 09:52:01 minikube kubelet[1594]: I1127 09:52:01.031635    1594 status_manager.go:851] "Failed to get status for pod" podUID="8988a6a8-f01f-40a4-901f-98465f71453c" pod="kube-system/coredns-6f6b679f8f-t7dd5" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/coredns-6f6b679f8f-t7dd5\": dial tcp 192.168.49.2:8443: connect: connection refused"
Nov 27 09:52:01 minikube kubelet[1594]: I1127 09:52:01.032030    1594 status_manager.go:851] "Failed to get status for pod" podUID="40f5f661ab65f2e4bfe41ac2993c01de" pod="kube-system/kube-controller-manager-minikube" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-controller-manager-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Nov 27 09:52:01 minikube kubelet[1594]: I1127 09:52:01.032574    1594 status_manager.go:851] "Failed to get status for pod" podUID="e492fec3-1add-4301-a139-814e669dcc6c" pod="kube-system/kube-proxy-njmkr" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-proxy-njmkr\": dial tcp 192.168.49.2:8443: connect: connection refused"
Nov 27 09:52:01 minikube kubelet[1594]: I1127 09:52:01.033004    1594 status_manager.go:851] "Failed to get status for pod" podUID="4cd61f82-ca99-482a-ab26-4c48b647912f" pod="kube-system/storage-provisioner" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/storage-provisioner\": dial tcp 192.168.49.2:8443: connect: connection refused"
Nov 27 09:52:01 minikube kubelet[1594]: I1127 09:52:01.128925    1594 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="d46a50ed87364d2eb028bd35668fb8eafe1377ddfb3e971a7608558878bc9541"
Nov 27 09:52:01 minikube kubelet[1594]: I1127 09:52:01.130149    1594 status_manager.go:851] "Failed to get status for pod" podUID="e039200acb850c82bb901653cc38ff6e" pod="kube-system/kube-scheduler-minikube" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-scheduler-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Nov 27 09:52:01 minikube kubelet[1594]: I1127 09:52:01.131252    1594 status_manager.go:851] "Failed to get status for pod" podUID="8988a6a8-f01f-40a4-901f-98465f71453c" pod="kube-system/coredns-6f6b679f8f-t7dd5" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/coredns-6f6b679f8f-t7dd5\": dial tcp 192.168.49.2:8443: connect: connection refused"
Nov 27 09:52:01 minikube kubelet[1594]: I1127 09:52:01.131846    1594 status_manager.go:851] "Failed to get status for pod" podUID="5c4b9b71-f203-48f9-ba4f-1d8ff42c9627" pod="prod/backend-5ff864f988-skjgt" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/prod/pods/backend-5ff864f988-skjgt\": dial tcp 192.168.49.2:8443: connect: connection refused"
Nov 27 09:52:01 minikube kubelet[1594]: I1127 09:52:01.132722    1594 status_manager.go:851] "Failed to get status for pod" podUID="e7ed82ff-a02b-44c2-8983-78642442c1b6" pod="prod/frontend-84cb8dc79f-jw9sj" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/prod/pods/frontend-84cb8dc79f-jw9sj\": dial tcp 192.168.49.2:8443: connect: connection refused"
Nov 27 09:52:01 minikube kubelet[1594]: I1127 09:52:01.133177    1594 status_manager.go:851] "Failed to get status for pod" podUID="a5363f4f31e043bdae3c93aca4991903" pod="kube-system/etcd-minikube" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/etcd-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Nov 27 09:52:01 minikube kubelet[1594]: I1127 09:52:01.213804    1594 status_manager.go:851] "Failed to get status for pod" podUID="9e315b3a91fa9f6f7463439d9dac1a56" pod="kube-system/kube-apiserver-minikube" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-apiserver-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Nov 27 09:52:01 minikube kubelet[1594]: I1127 09:52:01.214304    1594 status_manager.go:851] "Failed to get status for pod" podUID="4cd61f82-ca99-482a-ab26-4c48b647912f" pod="kube-system/storage-provisioner" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/storage-provisioner\": dial tcp 192.168.49.2:8443: connect: connection refused"
Nov 27 09:52:01 minikube kubelet[1594]: I1127 09:52:01.214526    1594 status_manager.go:851] "Failed to get status for pod" podUID="40f5f661ab65f2e4bfe41ac2993c01de" pod="kube-system/kube-controller-manager-minikube" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-controller-manager-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
Nov 27 09:52:01 minikube kubelet[1594]: I1127 09:52:01.215279    1594 status_manager.go:851] "Failed to get status for pod" podUID="e492fec3-1add-4301-a139-814e669dcc6c" pod="kube-system/kube-proxy-njmkr" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-proxy-njmkr\": dial tcp 192.168.49.2:8443: connect: connection refused"
Nov 27 09:52:02 minikube kubelet[1594]: I1127 09:52:02.545114    1594 kubelet_pods.go:1007] "Unable to retrieve pull secret, the image pull may not succeed." pod="prod/frontend-84cb8dc79f-jw9sj" secret="" err="secret \"efrei2023\" not found"
Nov 27 09:52:02 minikube kubelet[1594]: E1127 09:52:02.545607    1594 kuberuntime_manager.go:1272] "Unhandled Error" err="container &Container{Name:frontend-container,Image:efrei2023/frontend:$TAG,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mznxp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod frontend-84cb8dc79f-jw9sj_prod(e7ed82ff-a02b-44c2-8983-78642442c1b6): InvalidImageName: Failed to apply default image tag \"efrei2023/frontend:$TAG\": couldn't parse image name \"efrei2023/frontend:$TAG\": invalid reference format" logger="UnhandledError"
Nov 27 09:52:02 minikube kubelet[1594]: E1127 09:52:02.546881    1594 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"frontend-container\" with InvalidImageName: \"Failed to apply default image tag \\\"efrei2023/frontend:$TAG\\\": couldn't parse image name \\\"efrei2023/frontend:$TAG\\\": invalid reference format\"" pod="prod/frontend-84cb8dc79f-jw9sj" podUID="e7ed82ff-a02b-44c2-8983-78642442c1b6"
Nov 27 09:52:02 minikube kubelet[1594]: I1127 09:52:02.625243    1594 scope.go:117] "RemoveContainer" containerID="266291e5d4c00e0c329d18b5f00cb0816650c6512338e7d0fe3f4c1e05b9d1a0"
Nov 27 09:52:02 minikube kubelet[1594]: I1127 09:52:02.625807    1594 scope.go:117] "RemoveContainer" containerID="e3f12406ccce053f055c8da07ec7eeaff010943ddd42228c1399559948565eb3"
Nov 27 09:52:02 minikube kubelet[1594]: E1127 09:52:02.626080    1594 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 10s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(4cd61f82-ca99-482a-ab26-4c48b647912f)\"" pod="kube-system/storage-provisioner" podUID="4cd61f82-ca99-482a-ab26-4c48b647912f"
Nov 27 09:52:02 minikube kubelet[1594]: I1127 09:52:02.847733    1594 kubelet_pods.go:1007] "Unable to retrieve pull secret, the image pull may not succeed." pod="prod/backend-5ff864f988-skjgt" secret="" err="secret \"efrei2023\" not found"
Nov 27 09:52:02 minikube kubelet[1594]: E1127 09:52:02.847997    1594 kuberuntime_manager.go:1272] "Unhandled Error" err="container &Container{Name:backend-container,Image:efrei2023/backend:$TAG,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:5432,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:POSTGRES_PASSWORD,Value:postgres,ValueFrom:nil,},EnvVar{Name:POSTGRES_USER,Value:postgres,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-smkpr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod backend-5ff864f988-skjgt_prod(5c4b9b71-f203-48f9-ba4f-1d8ff42c9627): InvalidImageName: Failed to apply default image tag \"efrei2023/backend:$TAG\": couldn't parse image name \"efrei2023/backend:$TAG\": invalid reference format" logger="UnhandledError"
Nov 27 09:52:02 minikube kubelet[1594]: E1127 09:52:02.914069    1594 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"backend-container\" with InvalidImageName: \"Failed to apply default image tag \\\"efrei2023/backend:$TAG\\\": couldn't parse image name \\\"efrei2023/backend:$TAG\\\": invalid reference format\"" pod="prod/backend-5ff864f988-skjgt" podUID="5c4b9b71-f203-48f9-ba4f-1d8ff42c9627"
Nov 27 09:52:03 minikube kubelet[1594]: I1127 09:52:03.930740    1594 kubelet_pods.go:1007] "Unable to retrieve pull secret, the image pull may not succeed." pod="prod/frontend-84cb8dc79f-jw9sj" secret="" err="secret \"efrei2023\" not found"
Nov 27 09:52:03 minikube kubelet[1594]: E1127 09:52:03.930921    1594 kuberuntime_manager.go:1272] "Unhandled Error" err="container &Container{Name:frontend-container,Image:efrei2023/frontend:$TAG,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mznxp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod frontend-84cb8dc79f-jw9sj_prod(e7ed82ff-a02b-44c2-8983-78642442c1b6): InvalidImageName: Failed to apply default image tag \"efrei2023/frontend:$TAG\": couldn't parse image name \"efrei2023/frontend:$TAG\": invalid reference format" logger="UnhandledError"
Nov 27 09:52:03 minikube kubelet[1594]: E1127 09:52:03.932355    1594 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"frontend-container\" with InvalidImageName: \"Failed to apply default image tag \\\"efrei2023/frontend:$TAG\\\": couldn't parse image name \\\"efrei2023/frontend:$TAG\\\": invalid reference format\"" pod="prod/frontend-84cb8dc79f-jw9sj" podUID="e7ed82ff-a02b-44c2-8983-78642442c1b6"
Nov 27 09:52:14 minikube kubelet[1594]: I1127 09:52:14.647106    1594 kubelet_pods.go:1007] "Unable to retrieve pull secret, the image pull may not succeed." pod="prod/frontend-84cb8dc79f-jw9sj" secret="" err="secret \"efrei2023\" not found"
Nov 27 09:52:14 minikube kubelet[1594]: E1127 09:52:14.647540    1594 kuberuntime_manager.go:1272] "Unhandled Error" err="container &Container{Name:frontend-container,Image:efrei2023/frontend:$TAG,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mznxp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod frontend-84cb8dc79f-jw9sj_prod(e7ed82ff-a02b-44c2-8983-78642442c1b6): InvalidImageName: Failed to apply default image tag \"efrei2023/frontend:$TAG\": couldn't parse image name \"efrei2023/frontend:$TAG\": invalid reference format" logger="UnhandledError"
Nov 27 09:52:14 minikube kubelet[1594]: E1127 09:52:14.648946    1594 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"frontend-container\" with InvalidImageName: \"Failed to apply default image tag \\\"efrei2023/frontend:$TAG\\\": couldn't parse image name \\\"efrei2023/frontend:$TAG\\\": invalid reference format\"" pod="prod/frontend-84cb8dc79f-jw9sj" podUID="e7ed82ff-a02b-44c2-8983-78642442c1b6"
Nov 27 09:52:16 minikube kubelet[1594]: I1127 09:52:16.648138    1594 scope.go:117] "RemoveContainer" containerID="e3f12406ccce053f055c8da07ec7eeaff010943ddd42228c1399559948565eb3"
Nov 27 09:52:17 minikube kubelet[1594]: I1127 09:52:17.652394    1594 kubelet_pods.go:1007] "Unable to retrieve pull secret, the image pull may not succeed." pod="prod/backend-5ff864f988-skjgt" secret="" err="secret \"efrei2023\" not found"
Nov 27 09:52:17 minikube kubelet[1594]: E1127 09:52:17.652741    1594 kuberuntime_manager.go:1272] "Unhandled Error" err="container &Container{Name:backend-container,Image:efrei2023/backend:$TAG,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:5432,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:POSTGRES_PASSWORD,Value:postgres,ValueFrom:nil,},EnvVar{Name:POSTGRES_USER,Value:postgres,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-smkpr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod backend-5ff864f988-skjgt_prod(5c4b9b71-f203-48f9-ba4f-1d8ff42c9627): InvalidImageName: Failed to apply default image tag \"efrei2023/backend:$TAG\": couldn't parse image name \"efrei2023/backend:$TAG\": invalid reference format" logger="UnhandledError"
Nov 27 09:52:17 minikube kubelet[1594]: E1127 09:52:17.654452    1594 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"backend-container\" with InvalidImageName: \"Failed to apply default image tag \\\"efrei2023/backend:$TAG\\\": couldn't parse image name \\\"efrei2023/backend:$TAG\\\": invalid reference format\"" pod="prod/backend-5ff864f988-skjgt" podUID="5c4b9b71-f203-48f9-ba4f-1d8ff42c9627"
Nov 27 09:52:26 minikube kubelet[1594]: I1127 09:52:26.647845    1594 kubelet_pods.go:1007] "Unable to retrieve pull secret, the image pull may not succeed." pod="prod/frontend-84cb8dc79f-jw9sj" secret="" err="secret \"efrei2023\" not found"
Nov 27 09:52:26 minikube kubelet[1594]: E1127 09:52:26.648290    1594 kuberuntime_manager.go:1272] "Unhandled Error" err="container &Container{Name:frontend-container,Image:efrei2023/frontend:$TAG,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mznxp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod frontend-84cb8dc79f-jw9sj_prod(e7ed82ff-a02b-44c2-8983-78642442c1b6): InvalidImageName: Failed to apply default image tag \"efrei2023/frontend:$TAG\": couldn't parse image name \"efrei2023/frontend:$TAG\": invalid reference format" logger="UnhandledError"
Nov 27 09:52:26 minikube kubelet[1594]: E1127 09:52:26.650055    1594 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"frontend-container\" with InvalidImageName: \"Failed to apply default image tag \\\"efrei2023/frontend:$TAG\\\": couldn't parse image name \\\"efrei2023/frontend:$TAG\\\": invalid reference format\"" pod="prod/frontend-84cb8dc79f-jw9sj" podUID="e7ed82ff-a02b-44c2-8983-78642442c1b6"
Nov 27 09:52:31 minikube kubelet[1594]: I1127 09:52:31.646619    1594 kubelet_pods.go:1007] "Unable to retrieve pull secret, the image pull may not succeed." pod="prod/backend-5ff864f988-skjgt" secret="" err="secret \"efrei2023\" not found"
Nov 27 09:52:31 minikube kubelet[1594]: E1127 09:52:31.646948    1594 kuberuntime_manager.go:1272] "Unhandled Error" err="container &Container{Name:backend-container,Image:efrei2023/backend:$TAG,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:5432,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:POSTGRES_PASSWORD,Value:postgres,ValueFrom:nil,},EnvVar{Name:POSTGRES_USER,Value:postgres,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-smkpr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod backend-5ff864f988-skjgt_prod(5c4b9b71-f203-48f9-ba4f-1d8ff42c9627): InvalidImageName: Failed to apply default image tag \"efrei2023/backend:$TAG\": couldn't parse image name \"efrei2023/backend:$TAG\": invalid reference format" logger="UnhandledError"
Nov 27 09:52:31 minikube kubelet[1594]: E1127 09:52:31.648073    1594 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"backend-container\" with InvalidImageName: \"Failed to apply default image tag \\\"efrei2023/backend:$TAG\\\": couldn't parse image name \\\"efrei2023/backend:$TAG\\\": invalid reference format\"" pod="prod/backend-5ff864f988-skjgt" podUID="5c4b9b71-f203-48f9-ba4f-1d8ff42c9627"
Nov 27 09:52:41 minikube kubelet[1594]: I1127 09:52:41.645539    1594 kubelet_pods.go:1007] "Unable to retrieve pull secret, the image pull may not succeed." pod="prod/frontend-84cb8dc79f-jw9sj" secret="" err="secret \"efrei2023\" not found"
Nov 27 09:52:41 minikube kubelet[1594]: E1127 09:52:41.645836    1594 kuberuntime_manager.go:1272] "Unhandled Error" err="container &Container{Name:frontend-container,Image:efrei2023/frontend:$TAG,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mznxp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod frontend-84cb8dc79f-jw9sj_prod(e7ed82ff-a02b-44c2-8983-78642442c1b6): InvalidImageName: Failed to apply default image tag \"efrei2023/frontend:$TAG\": couldn't parse image name \"efrei2023/frontend:$TAG\": invalid reference format" logger="UnhandledError"
Nov 27 09:52:41 minikube kubelet[1594]: E1127 09:52:41.647031    1594 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"frontend-container\" with InvalidImageName: \"Failed to apply default image tag \\\"efrei2023/frontend:$TAG\\\": couldn't parse image name \\\"efrei2023/frontend:$TAG\\\": invalid reference format\"" pod="prod/frontend-84cb8dc79f-jw9sj" podUID="e7ed82ff-a02b-44c2-8983-78642442c1b6"
Nov 27 09:52:46 minikube kubelet[1594]: I1127 09:52:46.645796    1594 kubelet_pods.go:1007] "Unable to retrieve pull secret, the image pull may not succeed." pod="prod/backend-5ff864f988-skjgt" secret="" err="secret \"efrei2023\" not found"
Nov 27 09:52:46 minikube kubelet[1594]: E1127 09:52:46.646197    1594 kuberuntime_manager.go:1272] "Unhandled Error" err="container &Container{Name:backend-container,Image:efrei2023/backend:$TAG,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:5432,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:POSTGRES_PASSWORD,Value:postgres,ValueFrom:nil,},EnvVar{Name:POSTGRES_USER,Value:postgres,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-smkpr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod backend-5ff864f988-skjgt_prod(5c4b9b71-f203-48f9-ba4f-1d8ff42c9627): InvalidImageName: Failed to apply default image tag \"efrei2023/backend:$TAG\": couldn't parse image name \"efrei2023/backend:$TAG\": invalid reference format" logger="UnhandledError"
Nov 27 09:52:46 minikube kubelet[1594]: E1127 09:52:46.647483    1594 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"backend-container\" with InvalidImageName: \"Failed to apply default image tag \\\"efrei2023/backend:$TAG\\\": couldn't parse image name \\\"efrei2023/backend:$TAG\\\": invalid reference format\"" pod="prod/backend-5ff864f988-skjgt" podUID="5c4b9b71-f203-48f9-ba4f-1d8ff42c9627"
Nov 27 09:52:55 minikube kubelet[1594]: I1127 09:52:55.645640    1594 kubelet_pods.go:1007] "Unable to retrieve pull secret, the image pull may not succeed." pod="prod/frontend-84cb8dc79f-jw9sj" secret="" err="secret \"efrei2023\" not found"
Nov 27 09:52:55 minikube kubelet[1594]: E1127 09:52:55.646013    1594 kuberuntime_manager.go:1272] "Unhandled Error" err="container &Container{Name:frontend-container,Image:efrei2023/frontend:$TAG,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mznxp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod frontend-84cb8dc79f-jw9sj_prod(e7ed82ff-a02b-44c2-8983-78642442c1b6): InvalidImageName: Failed to apply default image tag \"efrei2023/frontend:$TAG\": couldn't parse image name \"efrei2023/frontend:$TAG\": invalid reference format" logger="UnhandledError"
Nov 27 09:52:55 minikube kubelet[1594]: E1127 09:52:55.648031    1594 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"frontend-container\" with InvalidImageName: \"Failed to apply default image tag \\\"efrei2023/frontend:$TAG\\\": couldn't parse image name \\\"efrei2023/frontend:$TAG\\\": invalid reference format\"" pod="prod/frontend-84cb8dc79f-jw9sj" podUID="e7ed82ff-a02b-44c2-8983-78642442c1b6"
Nov 27 09:52:59 minikube kubelet[1594]: I1127 09:52:59.645272    1594 kubelet_pods.go:1007] "Unable to retrieve pull secret, the image pull may not succeed." pod="prod/backend-5ff864f988-skjgt" secret="" err="secret \"efrei2023\" not found"
Nov 27 09:52:59 minikube kubelet[1594]: E1127 09:52:59.645668    1594 kuberuntime_manager.go:1272] "Unhandled Error" err="container &Container{Name:backend-container,Image:efrei2023/backend:$TAG,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:5432,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:POSTGRES_PASSWORD,Value:postgres,ValueFrom:nil,},EnvVar{Name:POSTGRES_USER,Value:postgres,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-smkpr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod backend-5ff864f988-skjgt_prod(5c4b9b71-f203-48f9-ba4f-1d8ff42c9627): InvalidImageName: Failed to apply default image tag \"efrei2023/backend:$TAG\": couldn't parse image name \"efrei2023/backend:$TAG\": invalid reference format" logger="UnhandledError"
Nov 27 09:52:59 minikube kubelet[1594]: E1127 09:52:59.646764    1594 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"backend-container\" with InvalidImageName: \"Failed to apply default image tag \\\"efrei2023/backend:$TAG\\\": couldn't parse image name \\\"efrei2023/backend:$TAG\\\": invalid reference format\"" pod="prod/backend-5ff864f988-skjgt" podUID="5c4b9b71-f203-48f9-ba4f-1d8ff42c9627"
Nov 27 09:53:07 minikube kubelet[1594]: I1127 09:53:07.643151    1594 kubelet_pods.go:1007] "Unable to retrieve pull secret, the image pull may not succeed." pod="prod/frontend-84cb8dc79f-jw9sj" secret="" err="secret \"efrei2023\" not found"
Nov 27 09:53:07 minikube kubelet[1594]: E1127 09:53:07.643494    1594 kuberuntime_manager.go:1272] "Unhandled Error" err="container &Container{Name:frontend-container,Image:efrei2023/frontend:$TAG,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mznxp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod frontend-84cb8dc79f-jw9sj_prod(e7ed82ff-a02b-44c2-8983-78642442c1b6): InvalidImageName: Failed to apply default image tag \"efrei2023/frontend:$TAG\": couldn't parse image name \"efrei2023/frontend:$TAG\": invalid reference format" logger="UnhandledError"
Nov 27 09:53:07 minikube kubelet[1594]: E1127 09:53:07.644647    1594 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"frontend-container\" with InvalidImageName: \"Failed to apply default image tag \\\"efrei2023/frontend:$TAG\\\": couldn't parse image name \\\"efrei2023/frontend:$TAG\\\": invalid reference format\"" pod="prod/frontend-84cb8dc79f-jw9sj" podUID="e7ed82ff-a02b-44c2-8983-78642442c1b6"
Nov 27 09:53:11 minikube kubelet[1594]: I1127 09:53:11.643813    1594 kubelet_pods.go:1007] "Unable to retrieve pull secret, the image pull may not succeed." pod="prod/backend-5ff864f988-skjgt" secret="" err="secret \"efrei2023\" not found"
Nov 27 09:53:11 minikube kubelet[1594]: E1127 09:53:11.644112    1594 kuberuntime_manager.go:1272] "Unhandled Error" err="container &Container{Name:backend-container,Image:efrei2023/backend:$TAG,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:5432,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:POSTGRES_PASSWORD,Value:postgres,ValueFrom:nil,},EnvVar{Name:POSTGRES_USER,Value:postgres,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-smkpr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod backend-5ff864f988-skjgt_prod(5c4b9b71-f203-48f9-ba4f-1d8ff42c9627): InvalidImageName: Failed to apply default image tag \"efrei2023/backend:$TAG\": couldn't parse image name \"efrei2023/backend:$TAG\": invalid reference format" logger="UnhandledError"
Nov 27 09:53:11 minikube kubelet[1594]: E1127 09:53:11.645245    1594 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"backend-container\" with InvalidImageName: \"Failed to apply default image tag \\\"efrei2023/backend:$TAG\\\": couldn't parse image name \\\"efrei2023/backend:$TAG\\\": invalid reference format\"" pod="prod/backend-5ff864f988-skjgt" podUID="5c4b9b71-f203-48f9-ba4f-1d8ff42c9627"


==> storage-provisioner [43cb30f1c9fb] <==
I1127 09:52:16.823309       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I1127 09:52:16.834346       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I1127 09:52:16.834628       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I1127 09:52:34.250994       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I1127 09:52:34.251662       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_c0fd367f-f9f7-4d8c-9183-4fbbe1ba4dde!
I1127 09:52:34.251600       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"e0e6819a-72ee-47f4-997a-ae11ad274a11", APIVersion:"v1", ResourceVersion:"13832", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_c0fd367f-f9f7-4d8c-9183-4fbbe1ba4dde became leader
I1127 09:52:34.351399       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_c0fd367f-f9f7-4d8c-9183-4fbbe1ba4dde!


==> storage-provisioner [e3f12406ccce] <==
I1127 09:52:00.419259       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F1127 09:52:00.628865       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: connect: connection refused

